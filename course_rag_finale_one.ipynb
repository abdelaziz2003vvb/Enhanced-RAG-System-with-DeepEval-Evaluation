{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdelaziz2003vvb/Enhanced-RAG-System-with-DeepEval-Evaluation/blob/main/course_rag_finale_one.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# SECTION 1: INSTALLATION & IMPORTS\n",
        "# =============================================================="
      ],
      "metadata": {
        "id": "H_iKaVrYNQI2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX7hXsr1NGg5",
        "outputId": "e62a8b93-bfdf-4b64-a528-6966c6aa3725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Installing required packages...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m319.0/319.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m328.2/328.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… All packages installed successfully!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All imports successful!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ“¦ Installing required packages...\")\n",
        "\n",
        "!pip install -q langchain langchain-community langchain-cohere langchain-text-splitters\n",
        "!pip install -q cohere faiss-cpu pypdf python-docx\n",
        "!pip install -q sentence-transformers tiktoken\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\\n\")\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "from google.colab import files, drive\n",
        "\n",
        "# LangChain\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,\n",
        "    Docx2txtLoader,\n",
        "    TextLoader,\n",
        "    DirectoryLoader\n",
        ")\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_cohere import CohereEmbeddings, ChatCohere\n",
        "\n",
        "# Cohere\n",
        "import cohere\n",
        "\n",
        "print(\"âœ… All imports successful!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# SECTION 2: CONFIGURATION\n",
        "# ================================================================="
      ],
      "metadata": {
        "id": "2dWddU8lN4qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    \"\"\"Configuration for RAG System\"\"\"\n",
        "\n",
        "    # API Keys\n",
        "    COHERE_API_KEY = \"ViqgIQ4U0D64Md9QFeMaInVvD5m0cSX2bhvA8otF\"\n",
        "\n",
        "    # Models\n",
        "    COHERE_MODEL = \"command-r-plus-08-2024\"\n",
        "    COHERE_EMBEDDING_MODEL = \"embed-english-v3.0\"\n",
        "    COHERE_RERANK_MODEL = \"rerank-english-v3.0\"\n",
        "\n",
        "    # Chunking Parameters\n",
        "    CHUNK_SIZE = 1000\n",
        "    CHUNK_OVERLAP = 200\n",
        "\n",
        "    # Retrieval Parameters\n",
        "    TOP_K = 10  # Retrieve more for reranking\n",
        "    TOP_K_RERANK = 5  # Final number after reranking\n",
        "\n",
        "    # FAISS Parameters\n",
        "    FAISS_INDEX_TYPE = \"IndexFlatL2\"  # Exact search (or \"IndexIVFFlat\" for faster ANN)\n",
        "\n",
        "    # Paths\n",
        "    VECTOR_STORE_PATH = \"./faiss_index\"\n",
        "    UPLOAD_FOLDER = \"./uploaded_documents\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Set environment variable\n",
        "os.environ[\"COHERE_API_KEY\"] = config.COHERE_API_KEY\n",
        "\n",
        "print(\"âš™ï¸ Configuration loaded successfully!\")\n",
        "print(f\"   - Model: {config.COHERE_MODEL}\")\n",
        "print(f\"   - Embedding Model: {config.COHERE_EMBEDDING_MODEL}\")\n",
        "print(f\"   - Chunk Size: {config.CHUNK_SIZE}\")\n",
        "print(f\"   - Top K Retrieval: {config.TOP_K}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5igYMlR5Nym3",
        "outputId": "8c22bfc9-5df2-43c1-bd4b-a46d42fc4a42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸ Configuration loaded successfully!\n",
            "   - Model: command-r-plus-08-2024\n",
            "   - Embedding Model: embed-english-v3.0\n",
            "   - Chunk Size: 1000\n",
            "   - Top K Retrieval: 10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# SECTION 3: DOCUMENT LOADING\n",
        "# =================================================================\n"
      ],
      "metadata": {
        "id": "-cOxPFBHOPCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentLoader:\n",
        "    \"\"\"Load documents from various formats\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.supported_formats = ['.pdf', '.docx', '.txt', '.md']\n",
        "\n",
        "    def upload_files(self) -> List[str]:\n",
        "        \"\"\"Upload files from local computer\"\"\"\n",
        "        print(\"ğŸ“¤ Please upload your documents...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        # Create upload directory\n",
        "        os.makedirs(config.UPLOAD_FOLDER, exist_ok=True)\n",
        "\n",
        "        file_paths = []\n",
        "        for filename in uploaded.keys():\n",
        "            file_path = os.path.join(config.UPLOAD_FOLDER, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(uploaded[filename])\n",
        "            file_paths.append(file_path)\n",
        "            print(f\"   âœ… Uploaded: {filename}\")\n",
        "\n",
        "        return file_paths\n",
        "\n",
        "    def load_from_drive(self, drive_path: str) -> List[str]:\n",
        "        \"\"\"Load files from Google Drive\"\"\"\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        file_paths = []\n",
        "        for root, dirs, files in os.walk(drive_path):\n",
        "            for file in files:\n",
        "                if any(file.endswith(ext) for ext in self.supported_formats):\n",
        "                    file_paths.append(os.path.join(root, file))\n",
        "\n",
        "        print(f\"ğŸ“ Found {len(file_paths)} documents in Drive\")\n",
        "        return file_paths\n",
        "\n",
        "    def load_documents(self, file_paths: List[str]) -> List[Document]:\n",
        "        \"\"\"Load documents from file paths\"\"\"\n",
        "        all_documents = []\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            try:\n",
        "                # Select appropriate loader based on file extension\n",
        "                if file_path.endswith('.pdf'):\n",
        "                    loader = PyPDFLoader(file_path)\n",
        "                elif file_path.endswith('.docx'):\n",
        "                    loader = Docx2txtLoader(file_path)\n",
        "                elif file_path.endswith(('.txt', '.md')):\n",
        "                    loader = TextLoader(file_path)\n",
        "                else:\n",
        "                    print(f\"   âš ï¸ Unsupported format: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                docs = loader.load()\n",
        "\n",
        "                # Add metadata\n",
        "                for doc in docs:\n",
        "                    doc.metadata['source'] = os.path.basename(file_path)\n",
        "                    doc.metadata['file_path'] = file_path\n",
        "\n",
        "                all_documents.extend(docs)\n",
        "                print(f\"   âœ… Loaded: {os.path.basename(file_path)} ({len(docs)} pages)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   âŒ Error loading {file_path}: {str(e)}\")\n",
        "\n",
        "        print(f\"\\nğŸ“š Total documents loaded: {len(all_documents)}\\n\")\n",
        "        return all_documents"
      ],
      "metadata": {
        "id": "XJLpq9X5OKDH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# SECTION 4: TEXT CHUNKING\n",
        "# ========================================================="
      ],
      "metadata": {
        "id": "XTAHI_UdOZjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextChunker:\n",
        "    \"\"\"Split documents into chunks\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = config.CHUNK_SIZE,\n",
        "                 chunk_overlap: int = config.CHUNK_OVERLAP):\n",
        "        self.splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        # Remove excessive whitespace\n",
        "        text = \" \".join(text.split())\n",
        "        return text\n",
        "\n",
        "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks\"\"\"\n",
        "        print(\"âœ‚ï¸ Chunking documents...\")\n",
        "\n",
        "        # Clean documents\n",
        "        for doc in documents:\n",
        "            doc.page_content = self.clean_text(doc.page_content)\n",
        "\n",
        "        # Split into chunks\n",
        "        chunks = self.splitter.split_documents(documents)\n",
        "\n",
        "        # Add chunk metadata\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata['chunk_id'] = i\n",
        "            chunk.metadata['chunk_size'] = len(chunk.page_content)\n",
        "\n",
        "        print(f\"   âœ… Created {len(chunks)} chunks\")\n",
        "        print(f\"   ğŸ“Š Average chunk size: {np.mean([c.metadata['chunk_size'] for c in chunks]):.0f} characters\\n\")\n",
        "\n",
        "        return chunks\n"
      ],
      "metadata": {
        "id": "OHUNihYYOZCM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# SECTION 5: EMBEDDINGS & VECTOR STORE (FAISS)\n",
        "# =============================================================="
      ],
      "metadata": {
        "id": "WTDvONdUOoUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FAISSVectorStore:\n",
        "    \"\"\"Manage FAISS vector store with Cohere embeddings\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"ğŸ”§ Initializing Cohere Embeddings...\")\n",
        "        self.embeddings = CohereEmbeddings(\n",
        "            cohere_api_key=config.COHERE_API_KEY,\n",
        "            model=config.COHERE_EMBEDDING_MODEL\n",
        "        )\n",
        "        self.vectorstore = None\n",
        "        print(\"   âœ… Embeddings initialized\\n\")\n",
        "\n",
        "    def create_vectorstore(self, chunks: List[Document]) -> FAISS:\n",
        "        \"\"\"Create FAISS vector store from chunks\"\"\"\n",
        "        print(\"ğŸ”¢ Creating embeddings and building FAISS index...\")\n",
        "        print(f\"   Processing {len(chunks)} chunks...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Create FAISS vector store\n",
        "        self.vectorstore = FAISS.from_documents(\n",
        "            documents=chunks,\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print(f\"   âœ… FAISS index created in {elapsed_time:.2f} seconds\")\n",
        "        print(f\"   ğŸ“Š Index size: {self.vectorstore.index.ntotal} vectors\")\n",
        "        print(f\"   ğŸ“ Vector dimension: {self.vectorstore.index.d}\\n\")\n",
        "\n",
        "        return self.vectorstore\n",
        "\n",
        "    def save_vectorstore(self, path: str = config.VECTOR_STORE_PATH):\n",
        "        \"\"\"Save FAISS index to disk\"\"\"\n",
        "        if self.vectorstore is None:\n",
        "            raise ValueError(\"No vector store to save\")\n",
        "\n",
        "        print(f\"ğŸ’¾ Saving FAISS index to {path}...\")\n",
        "        self.vectorstore.save_local(path)\n",
        "        print(\"   âœ… Vector store saved\\n\")\n",
        "\n",
        "    def load_vectorstore(self, path: str = config.VECTOR_STORE_PATH):\n",
        "        \"\"\"Load FAISS index from disk\"\"\"\n",
        "        print(f\"ğŸ“‚ Loading FAISS index from {path}...\")\n",
        "\n",
        "        self.vectorstore = FAISS.load_local(\n",
        "            path,\n",
        "            self.embeddings,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "\n",
        "        print(f\"   âœ… Loaded {self.vectorstore.index.ntotal} vectors\\n\")\n",
        "        return self.vectorstore\n",
        "\n",
        "    def similarity_search(self, query: str, k: int = config.TOP_K) -> List[Document]:\n",
        "        \"\"\"Search for similar documents using FAISS ANN\"\"\"\n",
        "        if self.vectorstore is None:\n",
        "            raise ValueError(\"Vector store not initialized\")\n",
        "\n",
        "        return self.vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "    def similarity_search_with_score(self, query: str, k: int = config.TOP_K) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"Search with similarity scores\"\"\"\n",
        "        if self.vectorstore is None:\n",
        "            raise ValueError(\"Vector store not initialized\")\n",
        "\n",
        "        return self.vectorstore.similarity_search_with_score(query, k=k)"
      ],
      "metadata": {
        "id": "BgjeH4tAOi95"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# SECTION 6: RERANKING (COHERE)\n",
        "# ========================================================="
      ],
      "metadata": {
        "id": "0xIlqOGMOvD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CohereReranker:\n",
        "    \"\"\"Rerank retrieved documents using Cohere\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.client = cohere.Client(config.COHERE_API_KEY)\n",
        "        self.model = config.COHERE_RERANK_MODEL\n",
        "\n",
        "    def rerank(self, query: str, documents: List[Document],\n",
        "               top_k: int = config.TOP_K_RERANK) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"Rerank documents using Cohere reranking API\"\"\"\n",
        "\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        print(f\"ğŸ¯ Reranking {len(documents)} documents...\")\n",
        "\n",
        "        # Prepare documents for reranking\n",
        "        doc_texts = [doc.page_content for doc in documents]\n",
        "\n",
        "        try:\n",
        "            # Call Cohere rerank API\n",
        "            response = self.client.rerank(\n",
        "                model=self.model,\n",
        "                query=query,\n",
        "                documents=doc_texts,\n",
        "                top_n=min(top_k, len(documents)),\n",
        "                return_documents=False\n",
        "            )\n",
        "\n",
        "            # Create reranked results\n",
        "            reranked = []\n",
        "            for result in response.results:\n",
        "                doc_index = result.index\n",
        "                relevance_score = result.relevance_score\n",
        "\n",
        "                doc = documents[doc_index]\n",
        "                doc.metadata['rerank_score'] = relevance_score\n",
        "                doc.metadata['rerank_position'] = len(reranked) + 1\n",
        "\n",
        "                reranked.append((doc, relevance_score))\n",
        "\n",
        "            print(f\"   âœ… Reranked to top {len(reranked)} documents\\n\")\n",
        "            return reranked\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   âš ï¸ Reranking error: {e}\")\n",
        "            print(\"   ğŸ“ Returning original order\\n\")\n",
        "            return [(doc, 0.5) for doc in documents[:top_k]]"
      ],
      "metadata": {
        "id": "dL3If4h5OuiL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# SECTION 7: LLM GENERATION (COHERE)\n",
        "# =========================================================="
      ],
      "metadata": {
        "id": "s76MGLHqO5gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMGenerator:\n",
        "    \"\"\"Generate answers using Cohere LLM\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.llm = ChatCohere(\n",
        "            cohere_api_key=config.COHERE_API_KEY,\n",
        "            model=config.COHERE_MODEL,\n",
        "            temperature=0.3,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "\n",
        "    def generate_answer(self, query: str, context_docs: List[Document]) -> Dict:\n",
        "        \"\"\"Generate answer from query and context\"\"\"\n",
        "\n",
        "        # Prepare context\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"Document {i+1} (Source: {doc.metadata.get('source', 'Unknown')}):\\n{doc.page_content}\"\n",
        "            for i, doc in enumerate(context_docs)\n",
        "        ])\n",
        "\n",
        "        # Create prompt\n",
        "        prompt = f\"\"\"Use the following context from documents to answer the question at the end.\n",
        "If you don't know the answer based on the context, just say that you don't know, don't try to make up an answer.\n",
        "Always cite the source documents in your answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Detailed Answer:\"\"\"\n",
        "\n",
        "        print(\"ğŸ¤– Generating answer with Cohere LLM...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Generate response\n",
        "        response = self.llm.invoke(prompt)\n",
        "\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        print(f\"   âœ… Answer generated in {generation_time:.2f} seconds\\n\")\n",
        "\n",
        "        return {\n",
        "            \"answer\": response.content,\n",
        "            \"sources\": context_docs,\n",
        "            \"generation_time\": generation_time\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ZxARkrDeO2tp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# SECTION 8: COMPLETE RAG PIPELINE\n",
        "# =========================================================\n"
      ],
      "metadata": {
        "id": "Kf_M_gKePGzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGPipeline:\n",
        "    \"\"\"Complete RAG pipeline orchestration\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vector_store = FAISSVectorStore()\n",
        "        self.reranker = CohereReranker()\n",
        "        self.generator = LLMGenerator()\n",
        "        self.is_initialized = False\n",
        "\n",
        "    def ingest_documents(self, file_paths: List[str]):\n",
        "        \"\"\"Complete ingestion pipeline\"\"\"\n",
        "        print(\"=\"*70)\n",
        "        print(\"STEP 1: DOCUMENT LOADING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        loader = DocumentLoader()\n",
        "        documents = loader.load_documents(file_paths)\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"STEP 2: TEXT CHUNKING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        chunker = TextChunker()\n",
        "        chunks = chunker.chunk_documents(documents)\n",
        "\n",
        "        # FIX: Store chunk count in app_state\n",
        "        app_state.total_chunks = len(chunks)\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"STEP 3: CREATING EMBEDDINGS & FAISS INDEX\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        self.vector_store.create_vectorstore(chunks)\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"STEP 4: SAVING VECTOR STORE\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        self.vector_store.save_vectorstore()\n",
        "\n",
        "        self.is_initialized = True\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"âœ… INGESTION COMPLETE!\")\n",
        "        print(\"=\"*70)\n",
        "        print()\n",
        "\n",
        "    def load_existing_index(self):\n",
        "        \"\"\"Load existing FAISS index\"\"\"\n",
        "        self.vector_store.load_vectorstore()\n",
        "        self.is_initialized = True\n",
        "\n",
        "    def query(self, question: str, use_reranking: bool = True) -> Dict:\n",
        "        \"\"\"Complete RAG query pipeline\"\"\"\n",
        "\n",
        "        if not self.is_initialized:\n",
        "            raise ValueError(\"Pipeline not initialized. Run ingest_documents() first.\")\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(f\"QUERY: {question}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # STEP 1: Retrieve from FAISS\n",
        "        print(\"\\nğŸ“Š STEP 1: RETRIEVAL (FAISS ANN)\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        retrieval_start = time.time()\n",
        "        retrieved_docs = self.vector_store.similarity_search(question, k=config.TOP_K)\n",
        "        retrieval_time = time.time() - retrieval_start\n",
        "\n",
        "        print(f\"   Retrieved {len(retrieved_docs)} documents in {retrieval_time:.3f}s\")\n",
        "\n",
        "        # STEP 2: Rerank\n",
        "        if use_reranking:\n",
        "            print(\"\\nğŸ¯ STEP 2: RERANKING (COHERE)\")\n",
        "            print(\"-\"*70)\n",
        "\n",
        "            rerank_start = time.time()\n",
        "            reranked_docs = self.reranker.rerank(question, retrieved_docs, config.TOP_K_RERANK)\n",
        "            rerank_time = time.time() - rerank_start\n",
        "\n",
        "            final_docs = [doc for doc, score in reranked_docs]\n",
        "            print(f\"   Reranked to {len(final_docs)} documents in {rerank_time:.3f}s\")\n",
        "        else:\n",
        "            final_docs = retrieved_docs[:config.TOP_K_RERANK]\n",
        "            rerank_time = 0\n",
        "\n",
        "        # STEP 3: Generate answer\n",
        "        print(\"\\nğŸ¤– STEP 3: GENERATION (COHERE LLM)\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        result = self.generator.generate_answer(question, final_docs)\n",
        "\n",
        "        # Add timing information\n",
        "        result['retrieval_time'] = retrieval_time\n",
        "        result['rerank_time'] = rerank_time\n",
        "        result['total_time'] = retrieval_time + rerank_time + result['generation_time']\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"âœ… QUERY COMPLETE\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def display_result(self, result: Dict):\n",
        "        \"\"\"Display query result in a nice format\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"ğŸ“ ANSWER\")\n",
        "        print(\"=\"*70)\n",
        "        print(result['answer'])\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"ğŸ“š SOURCES\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        for i, doc in enumerate(result['sources'], 1):\n",
        "            print(f\"\\n{i}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
        "            print(f\"   Rerank Score: {doc.metadata.get('rerank_score', 'N/A')}\")\n",
        "            print(f\"   Content Preview: {doc.page_content[:200]}...\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"â±ï¸ PERFORMANCE METRICS\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"   Retrieval Time: {result['retrieval_time']:.3f}s\")\n",
        "        print(f\"   Rerank Time: {result['rerank_time']:.3f}s\")\n",
        "        print(f\"   Generation Time: {result['generation_time']:.3f}s\")\n",
        "        print(f\"   Total Time: {result['total_time']:.3f}s\")\n",
        "        print(\"=\"*70)"
      ],
      "metadata": {
        "id": "2JgdM-XEPGPT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#=============================================================================\n",
        "#SECTION 9:DEEPEVAL EVALUATION + HUMAN-IN-THE-LOOP EXTENSION\n",
        "#=================================================================\n"
      ],
      "metadata": {
        "id": "5OHsn1o3AAEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CELL 1: Install Deep eval"
      ],
      "metadata": {
        "id": "jOv7mRgPASU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --upgrade --quiet\n",
        "!pip install deepeval --upgrade --quiet --break-system-packages\n",
        "from datasets import Dataset\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n"
      ],
      "metadata": {
        "id": "aOG26oEmAHx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c7e013-2172-4290-875c-f7c9c0474489"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m794.9/794.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "rasterio 1.4.4 requires click!=8.2.*,>=4.0, but you have click 8.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CELL 2: Enhanced AppState with Feedback Storage"
      ],
      "metadata": {
        "id": "zGRdDWm0AiN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedAppState:\n",
        "    \"\"\"Extended AppState with evaluation and feedback capabilities\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Existing attributes\n",
        "        self.rag_pipeline = None\n",
        "        self.chat_history = []\n",
        "        self.query_stats = []\n",
        "        self.is_initialized = False\n",
        "        self.documents_loaded = 0\n",
        "        self.file_paths = []\n",
        "        self.total_chunks = 0\n",
        "\n",
        "        # NEW: Feedback storage\n",
        "        self.feedback_data = []\n",
        "        self.evaluation_results = []\n",
        "\n",
        "        # NEW: Ground truth Q&A pairs\n",
        "        self.ground_truth_qa = []\n",
        "\n",
        "    def add_feedback(self, query_id, feedback_type, rating=None, comment=None, corrected_answer=None):\n",
        "        \"\"\"Store user feedback\"\"\"\n",
        "        feedback = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'query_id': query_id,\n",
        "            'feedback_type': feedback_type,  # 'thumbs_up', 'thumbs_down', 'correction'\n",
        "            'rating': rating,  # 1-5 stars\n",
        "            'comment': comment,\n",
        "            'corrected_answer': corrected_answer,\n",
        "            'question': self.chat_history[query_id]['question'] if query_id < len(self.chat_history) else None,\n",
        "            'original_answer': self.chat_history[query_id]['answer'] if query_id < len(self.chat_history) else None\n",
        "        }\n",
        "        self.feedback_data.append(feedback)\n",
        "        print(f\"âœ… Feedback recorded: {feedback_type}\")\n",
        "        return feedback\n",
        "\n",
        "    def export_feedback(self, filename=\"feedback_data.json\"):\n",
        "        \"\"\"Export all feedback data\"\"\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.feedback_data, f, indent=2, ensure_ascii=False)\n",
        "        return filename\n",
        "\n",
        "    def get_feedback_summary(self):\n",
        "        \"\"\"Get feedback statistics\"\"\"\n",
        "        if not self.feedback_data:\n",
        "            return \"No feedback yet\"\n",
        "\n",
        "        thumbs_up = sum(1 for f in self.feedback_data if f['feedback_type'] == 'thumbs_up')\n",
        "        thumbs_down = sum(1 for f in self.feedback_data if f['feedback_type'] == 'thumbs_down')\n",
        "        corrections = sum(1 for f in self.feedback_data if f['feedback_type'] == 'correction')\n",
        "\n",
        "        ratings = [f['rating'] for f in self.feedback_data if f['rating'] is not None]\n",
        "        avg_rating = sum(ratings) / len(ratings) if ratings else 0\n",
        "\n",
        "        return {\n",
        "            'total_feedback': len(self.feedback_data),\n",
        "            'thumbs_up': thumbs_up,\n",
        "            'thumbs_down': thumbs_down,\n",
        "            'corrections': corrections,\n",
        "            'average_rating': avg_rating,\n",
        "            'satisfaction_rate': (thumbs_up / (thumbs_up + thumbs_down) * 100) if (thumbs_up + thumbs_down) > 0 else 0\n",
        "        }\n",
        "\n",
        "# Replace the existing app_state\n",
        "app_state = EnhancedAppState()"
      ],
      "metadata": {
        "id": "pcB76e9jAnaF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CELL 3 :DEEPEVAL IMPLEMENTATION - Complete Replacement for RAGAS\n"
      ],
      "metadata": {
        "id": "AcG4fQ8zAxKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ğŸš€ COMPLETE DEEPEVAL SETUP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Step 1: Imports\n",
        "print(\"\\nğŸ“¦ Step 1: Importing libraries...\")\n",
        "from deepeval.metrics import (\n",
        "    AnswerRelevancyMetric,\n",
        "    FaithfulnessMetric,\n",
        "    ContextualPrecisionMetric,\n",
        "    ContextualRecallMetric\n",
        ")\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "from langchain_cohere import ChatCohere\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "print(\"âœ… Imports complete\")\n",
        "\n",
        "# Step 2: Create Cohere Model for DeepEval\n",
        "print(\"\\nğŸ”§ Step 2: Creating Cohere model...\")\n",
        "\n",
        "class CohereModel(DeepEvalBaseLLM):\n",
        "    \"\"\"Custom Cohere model for DeepEval\"\"\"\n",
        "\n",
        "    def __init__(self, api_key, model=\"command-r-plus-08-2024\"):\n",
        "        self.model_name = model\n",
        "        self.llm = ChatCohere(\n",
        "            cohere_api_key=api_key,\n",
        "            model=model,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "    def load_model(self):\n",
        "        return self.llm\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "            return response.content\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Generate error: {e}\")\n",
        "            raise\n",
        "\n",
        "    async def a_generate(self, prompt: str) -> str:\n",
        "        return self.generate(prompt)\n",
        "\n",
        "    def get_model_name(self):\n",
        "        return self.model_name\n",
        "\n",
        "# Create the Cohere model\n",
        "cohere_model = CohereModel(api_key=config.COHERE_API_KEY)\n",
        "print(\"âœ… Cohere model created\")\n",
        "\n",
        "# Test the model\n",
        "print(\"   Testing model...\")\n",
        "try:\n",
        "    test_response = cohere_model.generate(\"Say 'OK' in one word\")\n",
        "    print(f\"   âœ… Model test: {test_response}\")\n",
        "except Exception as e:\n",
        "    print(f\"   âš ï¸ Model test failed: {e}\")\n",
        "\n",
        "# Step 3: Create DeepEval Evaluator\n",
        "print(\"\\nğŸ”§ Step 3: Creating DeepEval Evaluator...\")\n",
        "\n",
        "class DeepEvalEvaluator:\n",
        "    \"\"\"Fixed DeepEval evaluator with proper context handling\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "        # Initialize metrics\n",
        "        print(\"   Initializing Faithfulness...\")\n",
        "        self.faithfulness = FaithfulnessMetric(\n",
        "            threshold=0.7,\n",
        "            model=self.model,\n",
        "            include_reason=True\n",
        "        )\n",
        "\n",
        "        print(\"   Initializing Answer Relevancy...\")\n",
        "        self.answer_relevancy = AnswerRelevancyMetric(\n",
        "            threshold=0.7,\n",
        "            model=self.model,\n",
        "            include_reason=True\n",
        "        )\n",
        "\n",
        "        print(\"   Initializing Contextual Precision...\")\n",
        "        self.contextual_precision = ContextualPrecisionMetric(\n",
        "            threshold=0.7,\n",
        "            model=self.model,\n",
        "            include_reason=True\n",
        "        )\n",
        "\n",
        "        print(\"   Initializing Contextual Recall...\")\n",
        "        self.contextual_recall = ContextualRecallMetric(\n",
        "            threshold=0.7,\n",
        "            model=self.model,\n",
        "            include_reason=True\n",
        "        )\n",
        "\n",
        "        print(\"   âœ… All metrics initialized\")\n",
        "\n",
        "    def evaluate_single_query(self, question, answer, contexts, ground_truth=None):\n",
        "        \"\"\"\n",
        "        Evaluate a single query-answer pair using DeepEval\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ğŸ”¬ DEEPEVAL EVALUATION\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Question: {question[:80]}...\")\n",
        "\n",
        "        # Validate inputs\n",
        "        if not question or not answer:\n",
        "            print(\"âŒ Question or answer is empty!\")\n",
        "            return None\n",
        "\n",
        "        # Ensure contexts is a list of strings\n",
        "        if not isinstance(contexts, list):\n",
        "            contexts = [str(contexts)]\n",
        "\n",
        "        contexts = [str(c).strip() for c in contexts if c and str(c).strip()]\n",
        "\n",
        "        if not contexts:\n",
        "            print(\"âŒ No valid contexts provided!\")\n",
        "            return None\n",
        "\n",
        "        print(f\"ğŸ“„ Contexts: {len(contexts)} documents\")\n",
        "        print(f\"ğŸ“ Total context length: {sum(len(c) for c in contexts)} characters\")\n",
        "\n",
        "        has_ground_truth = ground_truth and str(ground_truth).strip()\n",
        "        if has_ground_truth:\n",
        "            print(f\"âœ… Ground truth provided\")\n",
        "        else:\n",
        "            print(f\"â„¹ï¸ No ground truth (using 2 basic metrics)\")\n",
        "\n",
        "        try:\n",
        "            # Create test case\n",
        "            test_case = LLMTestCase(\n",
        "                input=question,\n",
        "                actual_output=answer,\n",
        "                retrieval_context=contexts,\n",
        "                expected_output=ground_truth if has_ground_truth else None\n",
        "            )\n",
        "\n",
        "            print(f\"\\nâ³ Running DeepEval metrics...\")\n",
        "\n",
        "            # Run metrics\n",
        "            scores = {}\n",
        "\n",
        "            # Faithfulness\n",
        "            print(f\"   ğŸ“Š Evaluating Faithfulness...\")\n",
        "            try:\n",
        "                self.faithfulness.measure(test_case)\n",
        "                scores['faithfulness'] = self.faithfulness.score\n",
        "                print(f\"      âœ… Score: {self.faithfulness.score:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"      âŒ Failed: {str(e)[:100]}\")\n",
        "\n",
        "            # Answer Relevancy\n",
        "            print(f\"   ğŸ“Š Evaluating Answer Relevancy...\")\n",
        "            try:\n",
        "                self.answer_relevancy.measure(test_case)\n",
        "                scores['answer_relevancy'] = self.answer_relevancy.score\n",
        "                print(f\"      âœ… Score: {self.answer_relevancy.score:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"      âŒ Failed: {str(e)[:100]}\")\n",
        "\n",
        "            # Ground truth metrics if available\n",
        "            if has_ground_truth:\n",
        "                print(f\"   ğŸ“Š Evaluating Contextual Precision...\")\n",
        "                try:\n",
        "                    self.contextual_precision.measure(test_case)\n",
        "                    scores['contextual_precision'] = self.contextual_precision.score\n",
        "                    print(f\"      âœ… Score: {self.contextual_precision.score:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"      âŒ Failed: {str(e)[:100]}\")\n",
        "\n",
        "                print(f\"   ğŸ“Š Evaluating Contextual Recall...\")\n",
        "                try:\n",
        "                    self.contextual_recall.measure(test_case)\n",
        "                    scores['contextual_recall'] = self.contextual_recall.score\n",
        "                    print(f\"      âœ… Score: {self.contextual_recall.score:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"      âŒ Failed: {str(e)[:100]}\")\n",
        "\n",
        "            if not scores:\n",
        "                print(f\"\\nâŒ No metrics succeeded!\")\n",
        "                return None\n",
        "\n",
        "            print(f\"\\nâœ… Evaluation complete!\")\n",
        "            print(f\"\\nğŸ“Š FINAL SCORES:\")\n",
        "            for metric, score in scores.items():\n",
        "                emoji = \"âœ…\" if score >= 0.7 else \"âš ï¸\" if score >= 0.5 else \"âŒ\"\n",
        "                print(f\"   {emoji} {metric}: {score:.4f}\")\n",
        "\n",
        "            print(f\"{'='*70}\\n\")\n",
        "\n",
        "            return scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ EVALUATION ERROR:\")\n",
        "            print(f\"   {str(e)}\")\n",
        "            import traceback\n",
        "            print(f\"\\nğŸ“‹ Traceback: {traceback.format_exc()[:500]}...\")\n",
        "            print(f\"{'='*70}\\n\")\n",
        "            return None\n",
        "\n",
        "    def evaluate_batch(self, qa_pairs):\n",
        "        \"\"\"Evaluate multiple Q&A pairs\"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ğŸ”¬ BATCH DEEPEVAL EVALUATION\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Total pairs: {len(qa_pairs)}\")\n",
        "\n",
        "        # Validate inputs\n",
        "        valid_pairs = []\n",
        "        for i, qa in enumerate(qa_pairs):\n",
        "            if not qa.get('question') or not qa.get('answer'):\n",
        "                print(f\"âš ï¸ Skipping pair {i+1}: missing question or answer\")\n",
        "                continue\n",
        "\n",
        "            contexts = qa.get('contexts', [])\n",
        "            if not isinstance(contexts, list):\n",
        "                contexts = [str(contexts)]\n",
        "\n",
        "            contexts = [str(c).strip() for c in contexts if c and str(c).strip()]\n",
        "\n",
        "            if not contexts:\n",
        "                print(f\"âš ï¸ Skipping pair {i+1}: no valid contexts\")\n",
        "                continue\n",
        "\n",
        "            valid_pairs.append({\n",
        "                'question': qa['question'],\n",
        "                'answer': qa['answer'],\n",
        "                'contexts': contexts,\n",
        "                'ground_truth': qa.get('ground_truth', '')\n",
        "            })\n",
        "\n",
        "        if not valid_pairs:\n",
        "            print(\"âŒ No valid pairs to evaluate!\")\n",
        "            return None\n",
        "\n",
        "        print(f\"âœ… Valid pairs: {len(valid_pairs)}\")\n",
        "\n",
        "        # Evaluate each pair\n",
        "        all_scores = {\n",
        "            'faithfulness': [],\n",
        "            'answer_relevancy': []\n",
        "        }\n",
        "\n",
        "        has_ground_truth = all(\n",
        "            pair.get('ground_truth') and str(pair['ground_truth']).strip()\n",
        "            for pair in valid_pairs\n",
        "        )\n",
        "\n",
        "        if has_ground_truth:\n",
        "            all_scores['contextual_precision'] = []\n",
        "            all_scores['contextual_recall'] = []\n",
        "\n",
        "        for i, pair in enumerate(valid_pairs, 1):\n",
        "            print(f\"\\n[{i}/{len(valid_pairs)}] {pair['question'][:60]}...\")\n",
        "\n",
        "            scores = self.evaluate_single_query(\n",
        "                question=pair['question'],\n",
        "                answer=pair['answer'],\n",
        "                contexts=pair['contexts'],\n",
        "                ground_truth=pair.get('ground_truth')\n",
        "            )\n",
        "\n",
        "            if scores:\n",
        "                for metric, score in scores.items():\n",
        "                    if metric in all_scores:\n",
        "                        all_scores[metric].append(score)\n",
        "\n",
        "        # Calculate averages\n",
        "        avg_scores = {}\n",
        "        for metric, scores_list in all_scores.items():\n",
        "            if scores_list:\n",
        "                avg_scores[metric] = sum(scores_list) / len(scores_list)\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ğŸ“Š AVERAGE SCORES:\")\n",
        "        for metric, score in avg_scores.items():\n",
        "            emoji = \"âœ…\" if score >= 0.7 else \"âš ï¸\" if score >= 0.5 else \"âŒ\"\n",
        "            print(f\"   {emoji} {metric}: {score:.4f}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        return avg_scores\n",
        "\n",
        "    def generate_report(self, scores, output_file=None):\n",
        "        \"\"\"Generate detailed evaluation report\"\"\"\n",
        "\n",
        "        if not scores:\n",
        "            return \"âŒ No scores available\"\n",
        "\n",
        "        report = f\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                   DEEPEVAL EVALUATION REPORT                  â•‘\n",
        "â•‘                  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}                          â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "METRIC SCORES\n",
        "{'='*60}\n",
        "\n",
        "\"\"\"\n",
        "        for metric, score in scores.items():\n",
        "            status = \"âœ…\" if score >= 0.7 else \"âš ï¸\" if score >= 0.5 else \"âŒ\"\n",
        "            metric_name = metric.replace('_', ' ').title()\n",
        "            report += f\"{status} {metric_name:<30} {score:.4f}\\n\"\n",
        "\n",
        "        report += f\"\\n{'='*60}\\n\"\n",
        "\n",
        "        # Overall assessment\n",
        "        avg_score = sum(scores.values()) / len(scores)\n",
        "        report += f\"OVERALL SCORE: {avg_score:.4f}\\n\\n\"\n",
        "\n",
        "        if avg_score >= 0.8:\n",
        "            report += \"ğŸ† EXCELLENT - System performing very well!\\n\"\n",
        "        elif avg_score >= 0.7:\n",
        "            report += \"âœ… GOOD - System performing adequately\\n\"\n",
        "        elif avg_score >= 0.5:\n",
        "            report += \"âš ï¸ FAIR - System needs improvement\\n\"\n",
        "        else:\n",
        "            report += \"âŒ POOR - Significant improvements needed\\n\"\n",
        "\n",
        "        report += f\"{'='*60}\\n\"\n",
        "\n",
        "        if output_file:\n",
        "            try:\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    f.write(report)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return report\n",
        "\n",
        "\n",
        "# Create the evaluator\n",
        "evaluator = DeepEvalEvaluator(model=cohere_model)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… COMPLETE SETUP SUCCESSFUL!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“Š Available Metrics:\")\n",
        "print(\"   âœ… Faithfulness (always)\")\n",
        "print(\"   âœ… Answer Relevancy (always)\")\n",
        "print(\"   âœ… Contextual Precision (with ground truth)\")\n",
        "print(\"   âœ… Contextual Recall (with ground truth)\")\n",
        "print(\"\\nğŸ’¡ Ready to use!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Quick test\n",
        "print(\"\\nğŸ§ª Running quick test...\")\n",
        "if app_state.is_initialized:\n",
        "    try:\n",
        "        result = app_state.rag_pipeline.query(\"MFCC signifie quoi?\", use_reranking=True)\n",
        "        contexts = [doc.page_content for doc in result['sources']]\n",
        "\n",
        "        scores = evaluator.evaluate_single_query(\n",
        "            question=\"MFCC signifie quoi?\",\n",
        "            answer=result['answer'],\n",
        "            contexts=contexts,\n",
        "            ground_truth=\"MFCC est l'acronyme de Mel Frequency Cepstral Coefficients\"\n",
        "        )\n",
        "\n",
        "        if scores:\n",
        "            print(\"\\nğŸ‰ TEST SUCCESSFUL! Evaluator is working!\")\n",
        "        else:\n",
        "            print(\"\\nâš ï¸ Test completed but no scores returned\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš ï¸ Test error: {str(e)[:200]}\")\n",
        "else:\n",
        "    print(\"âš ï¸ System not initialized - skipping test\")\n",
        "    print(\"   Initialize system first, then test evaluation\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {
        "id": "WX1jcSz5A04h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff834084-5e4e-42cf-b252-c4ef236d1178"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸš€ COMPLETE DEEPEVAL SETUP\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¦ Step 1: Importing libraries...\n",
            "âœ… Imports complete\n",
            "\n",
            "ğŸ”§ Step 2: Creating Cohere model...\n",
            "âœ… Cohere model created\n",
            "   Testing model...\n",
            "   âœ… Model test: OK.\n",
            "\n",
            "ğŸ”§ Step 3: Creating DeepEval Evaluator...\n",
            "   Initializing Faithfulness...\n",
            "   Initializing Answer Relevancy...\n",
            "   Initializing Contextual Precision...\n",
            "   Initializing Contextual Recall...\n",
            "   âœ… All metrics initialized\n",
            "\n",
            "======================================================================\n",
            "âœ… COMPLETE SETUP SUCCESSFUL!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“Š Available Metrics:\n",
            "   âœ… Faithfulness (always)\n",
            "   âœ… Answer Relevancy (always)\n",
            "   âœ… Contextual Precision (with ground truth)\n",
            "   âœ… Contextual Recall (with ground truth)\n",
            "\n",
            "ğŸ’¡ Ready to use!\n",
            "======================================================================\n",
            "\n",
            "ğŸ§ª Running quick test...\n",
            "âš ï¸ System not initialized - skipping test\n",
            "   Initialize system first, then test evaluation\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CELL 4: Enhanced Query Function with Evaluation\n"
      ],
      "metadata": {
        "id": "4LvvAofsBJWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_with_evaluation(question, use_reranking, top_k, evaluate_response=False, ground_truth=None):\n",
        "    \"\"\"Enhanced query function with DeepEval evaluation\"\"\"\n",
        "\n",
        "    if not app_state.is_initialized:\n",
        "        return \"âŒ Please initialize the system first!\", None, \"System not initialized\", None, None\n",
        "\n",
        "    if not question or not question.strip():\n",
        "        return \"âŒ Please enter a question!\", None, \"Empty question\", None, None\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ğŸ” PROCESSING QUERY\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Reranking: {use_reranking}\")\n",
        "        print(f\"Evaluate: {evaluate_response}\")\n",
        "\n",
        "        # Query the RAG pipeline\n",
        "        print(f\"\\nâ³ Querying RAG pipeline...\")\n",
        "        result = app_state.rag_pipeline.query(question, use_reranking=use_reranking)\n",
        "\n",
        "        # Extract contexts as clean strings\n",
        "        contexts = []\n",
        "        for doc in result['sources']:\n",
        "            context_text = doc.page_content.strip()\n",
        "            if context_text:\n",
        "                contexts.append(context_text)\n",
        "\n",
        "        print(f\"âœ… Retrieved {len(contexts)} context documents\")\n",
        "\n",
        "        # Initialize evaluation variables\n",
        "        evaluation_scores = None\n",
        "        evaluation_report = \"â„¹ï¸ Evaluation not requested\"\n",
        "\n",
        "        # Run DeepEval evaluation if requested\n",
        "        if evaluate_response:\n",
        "            print(f\"\\nğŸ”¬ Running DeepEval evaluation...\")\n",
        "\n",
        "            # Clean ground truth\n",
        "            gt = None\n",
        "            if ground_truth:\n",
        "                gt = str(ground_truth).strip()\n",
        "                if gt:\n",
        "                    print(f\"âœ… Ground truth: {gt[:100]}...\")\n",
        "                else:\n",
        "                    print(f\"â„¹ï¸ Ground truth: (empty)\")\n",
        "\n",
        "            try:\n",
        "                evaluation_scores = evaluator.evaluate_single_query(\n",
        "                    question=question,\n",
        "                    answer=result['answer'],\n",
        "                    contexts=contexts,\n",
        "                    ground_truth=gt\n",
        "                )\n",
        "\n",
        "                if evaluation_scores:\n",
        "                    print(f\"\\nâœ… DeepEval evaluation completed!\")\n",
        "                    evaluation_report = evaluator.generate_report(evaluation_scores)\n",
        "\n",
        "                    # Store evaluation result\n",
        "                    app_state.evaluation_results.append({\n",
        "                        'timestamp': datetime.now().isoformat(),\n",
        "                        'question': question,\n",
        "                        'answer': result['answer'],\n",
        "                        'scores': evaluation_scores,\n",
        "                        'ground_truth': gt\n",
        "                    })\n",
        "                else:\n",
        "                    evaluation_report = \"\"\"\n",
        "âš ï¸ **Evaluation Failed**\n",
        "\n",
        "DeepEval did not return scores. Check console output for details.\n",
        "\n",
        "**Troubleshooting:**\n",
        "1. Verify Cohere API key is valid and has quota\n",
        "2. Check that contexts are not empty\n",
        "3. Try a simpler question first\n",
        "4. Review console for detailed error messages\n",
        "\"\"\"\n",
        "\n",
        "            except Exception as eval_error:\n",
        "                print(f\"\\nâŒ Evaluation exception: {str(eval_error)}\")\n",
        "                import traceback\n",
        "                print(traceback.format_exc())\n",
        "                evaluation_report = f\"\"\"\n",
        "âŒ **Evaluation Error**\n",
        "\n",
        "```\n",
        "{str(eval_error)}\n",
        "```\n",
        "\n",
        "**Check console for full traceback.**\n",
        "\n",
        "**Quick fixes:**\n",
        "1. Restart kernel and reload all cells\n",
        "2. Verify Cohere API key: `{config.COHERE_API_KEY[:10]}...`\n",
        "3. Check internet connection\n",
        "4. Try without ground truth first\n",
        "\"\"\"\n",
        "\n",
        "        # Format answer for display\n",
        "        answer_text = f\"\"\"\n",
        "## ğŸ’¡ Answer\n",
        "\n",
        "{result['answer']}\n",
        "\n",
        "---\n",
        "\n",
        "### â±ï¸ Performance Metrics\n",
        "\n",
        "- **Retrieval Time:** {result['retrieval_time']:.3f}s\n",
        "- **Reranking Time:** {result['rerank_time']:.3f}s\n",
        "- **Generation Time:** {result['generation_time']:.3f}s\n",
        "- **Total Time:** {result['total_time']:.3f}s\n",
        "- **Documents Retrieved:** {len(contexts)}\n",
        "\"\"\"\n",
        "\n",
        "        # Add evaluation scores if available\n",
        "        if evaluation_scores:\n",
        "            answer_text += f\"\"\"\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Š DeepEval Scores\n",
        "\n",
        "\"\"\"\n",
        "            for metric, score in evaluation_scores.items():\n",
        "                emoji = \"âœ…\" if score >= 0.7 else \"âš ï¸\" if score >= 0.5 else \"âŒ\"\n",
        "                metric_name = metric.replace('_', ' ').title()\n",
        "\n",
        "                # Special handling for hallucination (higher is better)\n",
        "                if metric == 'hallucination':\n",
        "                    answer_text += f\"- {emoji} **{metric_name}:** {score:.4f} (higher = less hallucination)\\n\"\n",
        "                else:\n",
        "                    answer_text += f\"- {emoji} **{metric_name}:** {score:.4f}\\n\"\n",
        "\n",
        "        # Format sources\n",
        "        sources_data = []\n",
        "        for i, doc in enumerate(result['sources'], 1):\n",
        "            sources_data.append({\n",
        "                'Rank': i,\n",
        "                'Source': doc.metadata.get('source', 'Unknown'),\n",
        "                'Rerank Score': f\"{doc.metadata.get('rerank_score', 0):.4f}\",\n",
        "                'Length': f\"{len(doc.page_content)} chars\",\n",
        "                'Preview': doc.page_content[:150] + \"...\"\n",
        "            })\n",
        "\n",
        "        sources_df = pd.DataFrame(sources_data) if sources_data else None\n",
        "\n",
        "        # Store in chat history\n",
        "        query_id = len(app_state.chat_history)\n",
        "        app_state.chat_history.append({\n",
        "            'query_id': query_id,\n",
        "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'question': question,\n",
        "            'answer': result['answer'],\n",
        "            'total_time': result['total_time'],\n",
        "            'evaluation_scores': evaluation_scores,\n",
        "            'contexts': contexts,\n",
        "            'ground_truth': ground_truth if ground_truth and str(ground_truth).strip() else None\n",
        "        })\n",
        "\n",
        "        print(f\"\\nâœ… Query complete! Query ID: {query_id}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        return answer_text, sources_df, evaluation_report, query_id, evaluation_scores\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_trace = traceback.format_exc()\n",
        "\n",
        "        print(f\"\\nâŒ QUERY ERROR:\")\n",
        "        print(error_trace)\n",
        "\n",
        "        error_msg = f\"\"\"\n",
        "## âŒ Query Error\n",
        "\n",
        "**Error:** {str(e)}\n",
        "\n",
        "**Traceback:**\n",
        "```\n",
        "{error_trace}\n",
        "```\n",
        "\n",
        "**Troubleshooting:**\n",
        "1. System initialization: {'âœ…' if app_state.is_initialized else 'âŒ'}\n",
        "2. Check console output above\n",
        "3. Verify Cohere API key\n",
        "4. Try restarting kernel\n",
        "\"\"\"\n",
        "        return error_msg, None, \"Error occurred\", None, None\n",
        "\n"
      ],
      "metadata": {
        "id": "OsU_L0M2BQT6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CELL 5: Feedback Collection Functions\n"
      ],
      "metadata": {
        "id": "Au22xXCSBYUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def record_thumbs_feedback(query_id, feedback_type):\n",
        "    \"\"\"Record thumbs up/down feedback\"\"\"\n",
        "\n",
        "    if query_id is None or query_id >= len(app_state.chat_history):\n",
        "        return \"âŒ Invalid query ID\"\n",
        "\n",
        "    app_state.add_feedback(\n",
        "        query_id=query_id,\n",
        "        feedback_type=feedback_type\n",
        "    )\n",
        "\n",
        "    return f\"âœ… Feedback recorded: {feedback_type}!\"\n",
        "\n",
        "def record_rating_feedback(query_id, rating, comment=\"\"):\n",
        "    \"\"\"Record star rating feedback\"\"\"\n",
        "\n",
        "    if query_id is None or query_id >= len(app_state.chat_history):\n",
        "        return \"âŒ Invalid query ID\"\n",
        "\n",
        "    app_state.add_feedback(\n",
        "        query_id=query_id,\n",
        "        feedback_type='rating',\n",
        "        rating=rating,\n",
        "        comment=comment\n",
        "    )\n",
        "\n",
        "    return f\"âœ… Rating recorded: {rating}/5 stars!\"\n",
        "\n",
        "def record_correction(query_id, corrected_answer, comment=\"\"):\n",
        "    \"\"\"Record corrected answer from user\"\"\"\n",
        "\n",
        "    if query_id is None or query_id >= len(app_state.chat_history):\n",
        "        return \"âŒ Invalid query ID\"\n",
        "\n",
        "    app_state.add_feedback(\n",
        "        query_id=query_id,\n",
        "        feedback_type='correction',\n",
        "        corrected_answer=corrected_answer,\n",
        "        comment=comment\n",
        "    )\n",
        "\n",
        "    # Add to ground truth\n",
        "    app_state.ground_truth_qa.append({\n",
        "        'question': app_state.chat_history[query_id]['question'],\n",
        "        'ground_truth': corrected_answer,\n",
        "        'contexts': app_state.chat_history[query_id].get('contexts', [])\n",
        "    })\n",
        "\n",
        "    return f\"âœ… Correction recorded and added to ground truth!\"\n",
        "\n",
        "def get_feedback_dashboard():\n",
        "    \"\"\"Generate feedback dashboard\"\"\"\n",
        "\n",
        "    try:\n",
        "        summary = app_state.get_feedback_summary()\n",
        "\n",
        "        # Handle case when there's no feedback\n",
        "        if isinstance(summary, str):\n",
        "            return f\"# ğŸ“Š Feedback Dashboard\\n\\n{summary}\"\n",
        "\n",
        "        dashboard = f\"\"\"\n",
        "# ğŸ“Š Feedback Dashboard\n",
        "\n",
        "## Overall Statistics\n",
        "\n",
        "- **Total Feedback Received:** {summary['total_feedback']}\n",
        "- **Thumbs Up:** ğŸ‘ {summary['thumbs_up']}\n",
        "- **Thumbs Down:** ğŸ‘ {summary['thumbs_down']}\n",
        "- **Corrections Provided:** âœï¸ {summary['corrections']}\n",
        "- **Average Rating:** â­ {summary['average_rating']:.2f}/5.0\n",
        "- **Satisfaction Rate:** {summary['satisfaction_rate']:.1f}%\n",
        "\n",
        "## Recent Feedback\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        # Show last 5 feedback items\n",
        "        if app_state.feedback_data:\n",
        "            for fb in app_state.feedback_data[-5:]:\n",
        "                dashboard += f\"\"\"\n",
        "---\n",
        "\n",
        "**{fb['timestamp']}** - {fb['feedback_type']}\n",
        "\n",
        "- **Question:** {fb['question'][:100] if fb['question'] else 'N/A'}...\n",
        "\"\"\"\n",
        "                if fb.get('rating'):\n",
        "                    dashboard += f\"- **Rating:** {'â­' * fb['rating']}\\n\"\n",
        "                if fb.get('comment'):\n",
        "                    dashboard += f\"- **Comment:** {fb['comment']}\\n\"\n",
        "                if fb.get('corrected_answer'):\n",
        "                    dashboard += f\"- **Correction:** {fb['corrected_answer'][:100]}...\\n\"\n",
        "        else:\n",
        "            dashboard += \"\\n*No feedback items yet*\\n\"\n",
        "\n",
        "        return dashboard\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_msg = f\"\"\"\n",
        "# âŒ Dashboard Error\n",
        "\n",
        "**Error:** {str(e)}\n",
        "\n",
        "**Traceback:**\n",
        "```\n",
        "{traceback.format_exc()}\n",
        "```\n",
        "\n",
        "**Quick Fix:** Restart kernel and try again.\n",
        "\"\"\"\n",
        "        return error_msg\n",
        "\n",
        "def export_all_data():\n",
        "    \"\"\"Export all feedback and evaluation data\"\"\"\n",
        "\n",
        "    try:\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Export feedback\n",
        "        feedback_file = f\"feedback_{timestamp}.json\"\n",
        "        with open(feedback_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(app_state.feedback_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Export evaluation results\n",
        "        eval_file = f\"evaluation_{timestamp}.json\"\n",
        "        with open(eval_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(app_state.evaluation_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Export ground truth\n",
        "        gt_file = f\"ground_truth_{timestamp}.json\"\n",
        "        with open(gt_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(app_state.ground_truth_qa, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        summary = f\"\"\"\n",
        "âœ… **Data Exported Successfully!**\n",
        "\n",
        "**Files created:**\n",
        "1. `{feedback_file}` - User feedback data\n",
        "2. `{eval_file}` - DeepEval evaluation results\n",
        "3. `{gt_file}` - Ground truth Q&A pairs\n",
        "\n",
        "**Total records:**\n",
        "- Feedback: {len(app_state.feedback_data)}\n",
        "- Evaluations: {len(app_state.evaluation_results)}\n",
        "- Ground Truth: {len(app_state.ground_truth_qa)}\n",
        "\n",
        "**Download the files using the file widgets below.**\n",
        "\"\"\"\n",
        "\n",
        "        return summary, feedback_file, eval_file, gt_file\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_msg = f\"\"\"\n",
        "âŒ **Export Failed**\n",
        "\n",
        "**Error:** {str(e)}\n",
        "```\n",
        "{traceback.format_exc()}\n",
        "```\n",
        "\"\"\"\n",
        "        return error_msg, None, None, None\n"
      ],
      "metadata": {
        "id": "Rgo70Ni_BcxK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CELL 6: Batch Evaluation Function"
      ],
      "metadata": {
        "id": "gLIMDs39Bq9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_batch_evaluation():\n",
        "    \"\"\"Run batch evaluation using DeepEval\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ğŸ”¬ BATCH DEEPEVAL EVALUATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Check chat history\n",
        "    print(f\"\\nğŸ“Š Checking chat history...\")\n",
        "    print(f\"   Total queries: {len(app_state.chat_history)}\")\n",
        "\n",
        "    # Find queries with ground truth\n",
        "    queries_with_gt = []\n",
        "    for i, query in enumerate(app_state.chat_history):\n",
        "        gt = query.get('ground_truth')\n",
        "        if gt and str(gt).strip():\n",
        "            print(f\"   âœ… Query {i}: Has ground truth\")\n",
        "            queries_with_gt.append(query)\n",
        "        else:\n",
        "            print(f\"   âš ï¸ Query {i}: No ground truth\")\n",
        "\n",
        "    print(f\"\\nğŸ“ˆ Summary:\")\n",
        "    print(f\"   Total queries: {len(app_state.chat_history)}\")\n",
        "    print(f\"   With ground truth: {len(queries_with_gt)}\")\n",
        "\n",
        "    if len(queries_with_gt) == 0:\n",
        "        return f\"\"\"\n",
        "âŒ **No Ground Truth Data Available**\n",
        "\n",
        "**Current Status:**\n",
        "- Total queries in history: {len(app_state.chat_history)}\n",
        "- Queries with ground truth: 0\n",
        "\n",
        "**How to fix this:**\n",
        "\n",
        "### Option 1: Evaluate Queries with Ground Truth\n",
        "1. Go to **\"Ask & Evaluate\"** tab\n",
        "2. Enter a question (e.g., \"MFCC signifie quoi?\")\n",
        "3. Check **âœ“ \"Evaluate with DeepEval\"**\n",
        "4. Enter ground truth: \"MFCC est l'acronyme de Mel Frequency Cepstral Coefficients\"\n",
        "5. Click **\"Get Answer\"**\n",
        "6. Repeat 3-5 times with different questions\n",
        "7. Return here and click **\"Run Batch Evaluation\"**\n",
        "\n",
        "### Option 2: Provide Corrections\n",
        "1. Ask some questions normally (without evaluation)\n",
        "2. For wrong answers, use **\"Provide Correction\"** section\n",
        "3. Enter the correct answer\n",
        "4. Submit 3-5 corrections\n",
        "5. Return here and click **\"Run Batch Evaluation\"**\n",
        "\n",
        "**Tip:** You need at least 1 query with ground truth to run batch evaluation.\n",
        "\n",
        "**Debug Info:**\n",
        "- Queries evaluated: {len([q for q in app_state.chat_history if q.get('evaluation_scores')])}\n",
        "- Corrections provided: {len([q for q in app_state.chat_history if q.get('ground_truth')])}\n",
        "\"\"\", None\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nğŸ“ Preparing {len(queries_with_gt)} queries for batch evaluation...\")\n",
        "\n",
        "        # Prepare Q&A pairs\n",
        "        qa_pairs = []\n",
        "        for i, query in enumerate(queries_with_gt, 1):\n",
        "            print(f\"\\n   [{i}/{len(queries_with_gt)}]\")\n",
        "            print(f\"   Question: {query['question'][:60]}...\")\n",
        "            print(f\"   Ground truth: {query['ground_truth'][:60]}...\")\n",
        "\n",
        "            # Get contexts\n",
        "            if 'contexts' in query and query['contexts']:\n",
        "                contexts = query['contexts']\n",
        "                print(f\"   âœ… Using stored contexts ({len(contexts)})\")\n",
        "            else:\n",
        "                print(f\"   â³ Re-querying to get fresh contexts...\")\n",
        "                try:\n",
        "                    result = app_state.rag_pipeline.query(query['question'])\n",
        "                    contexts = [doc.page_content for doc in result['sources']]\n",
        "                    query['answer'] = result['answer']\n",
        "                    print(f\"   âœ… Got {len(contexts)} fresh contexts\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   âŒ Re-query failed: {str(e)[:100]}\")\n",
        "                    continue\n",
        "\n",
        "            # Validate contexts\n",
        "            contexts = [str(c).strip() for c in contexts if c and str(c).strip()]\n",
        "\n",
        "            if not contexts:\n",
        "                print(f\"   âŒ Skipping: no valid contexts\")\n",
        "                continue\n",
        "\n",
        "            # Add to batch\n",
        "            qa_pairs.append({\n",
        "                'question': query['question'],\n",
        "                'answer': query['answer'],\n",
        "                'contexts': contexts,\n",
        "                'ground_truth': query['ground_truth']\n",
        "            })\n",
        "            print(f\"   âœ… Added to batch\")\n",
        "\n",
        "        if not qa_pairs:\n",
        "            return \"\"\"\n",
        "âŒ **No Valid Queries for Batch Evaluation**\n",
        "\n",
        "All queries had issues (missing contexts, answers, etc.)\n",
        "\n",
        "**Try:**\n",
        "1. Submit 2-3 new queries with evaluation enabled\n",
        "2. Make sure to provide ground truth answers\n",
        "3. Click \"Get Answer\" and wait for results\n",
        "4. Then run batch evaluation again\n",
        "\"\"\", None\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"â³ Running DeepEval on {len(qa_pairs)} queries...\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Run batch evaluation using the evaluator\n",
        "        scores = evaluator.evaluate_batch(qa_pairs)\n",
        "\n",
        "        if scores and len(scores) > 0:\n",
        "            # Generate report\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            report_file = f\"deepeval_batch_{timestamp}.txt\"\n",
        "            report = evaluator.generate_report(scores, report_file)\n",
        "\n",
        "            # Add header with details\n",
        "            header = f\"\"\"\n",
        "# ğŸ¯ DeepEval Batch Evaluation Results\n",
        "\n",
        "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "**Queries Evaluated:** {len(qa_pairs)}\n",
        "**Total in History:** {len(app_state.chat_history)}\n",
        "**Evaluation Tool:** DeepEval\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "            final_report = header + report\n",
        "\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"âœ… BATCH EVALUATION COMPLETE!\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"ğŸ“„ Report saved: {report_file}\")\n",
        "            print(f\"ğŸ“Š Queries evaluated: {len(qa_pairs)}\")\n",
        "            print(f\"\\nğŸ“ˆ Results:\")\n",
        "            for metric, score in scores.items():\n",
        "                emoji = \"âœ…\" if score >= 0.7 else \"âš ï¸\" if score >= 0.5 else \"âŒ\"\n",
        "                print(f\"   {emoji} {metric}: {score:.4f}\")\n",
        "            print(f\"{'='*70}\\n\")\n",
        "\n",
        "            return final_report, report_file\n",
        "        else:\n",
        "            return f\"\"\"\n",
        "âŒ **Batch Evaluation Failed**\n",
        "\n",
        "DeepEval did not produce scores.\n",
        "\n",
        "**What happened:**\n",
        "- Prepared {len(qa_pairs)} queries\n",
        "- All had ground truth\n",
        "- But evaluation returned no scores\n",
        "\n",
        "**Troubleshooting:**\n",
        "1. Check console output above for errors\n",
        "2. Verify Cohere API is working (check usage)\n",
        "3. Try evaluating just 1 query first\n",
        "4. Wait 5 minutes and try again (rate limits)\n",
        "\n",
        "**Debug:**\n",
        "- Queries prepared: {len(qa_pairs)}\n",
        "- Scores returned: {scores}\n",
        "\"\"\", None\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_trace = traceback.format_exc()\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"âŒ BATCH EVALUATION ERROR\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(error_trace)\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        return f\"\"\"\n",
        "âŒ **Batch Evaluation Error**\n",
        "\n",
        "**Error:** {str(e)}\n",
        "\n",
        "**Traceback:**\n",
        "```\n",
        "{error_trace[:500]}\n",
        "```\n",
        "\n",
        "**Common Causes:**\n",
        "1. **API Rate Limit:** Wait 5-10 minutes\n",
        "2. **No Ground Truth:** Provide ground truth in queries\n",
        "3. **Empty Contexts:** Documents not loaded properly\n",
        "4. **API Key Issue:** Check Cohere dashboard\n",
        "\n",
        "**Quick Fix:**\n",
        "1. Check console output above for detailed error\n",
        "2. Try evaluating a single query first\n",
        "3. Verify ground truth is provided\n",
        "4. Make sure contexts are not empty\n",
        "\n",
        "**Need Help?**\n",
        "Share the full error from console for debugging.\n",
        "\"\"\", None\n",
        "\n",
        "\n",
        "print(\"âœ… Fixed batch evaluation function loaded!\")\n",
        "print(\"\\nğŸ’¡ To use batch evaluation:\")\n",
        "print(\"   1. Submit 3-5 queries with ground truth\")\n",
        "print(\"   2. Go to 'Batch Evaluation' tab\")\n",
        "print(\"   3. Click 'Run Batch Evaluation'\")\n",
        "print(\"\\nğŸ” The function will:\")\n",
        "print(\"   â€¢ Find all queries with ground truth\")\n",
        "print(\"   â€¢ Re-evaluate them with DeepEval\")\n",
        "print(\"   â€¢ Show average scores across all queries\")\n",
        "print(\"   â€¢ Generate a comprehensive report\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PDMb9QtBolj",
        "outputId": "7500ba40-7c69-45aa-ad3f-f8094918bfbf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fixed batch evaluation function loaded!\n",
            "\n",
            "ğŸ’¡ To use batch evaluation:\n",
            "   1. Submit 3-5 queries with ground truth\n",
            "   2. Go to 'Batch Evaluation' tab\n",
            "   3. Click 'Run Batch Evaluation'\n",
            "\n",
            "ğŸ” The function will:\n",
            "   â€¢ Find all queries with ground truth\n",
            "   â€¢ Re-evaluate them with DeepEval\n",
            "   â€¢ Show average scores across all queries\n",
            "   â€¢ Generate a comprehensive report\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CELL 7: helper : ADD GROUND TRUTH TO EXISTING QUERIES"
      ],
      "metadata": {
        "id": "5kw-xedsAw5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_ground_truth_to_query(query_id, ground_truth):\n",
        "    \"\"\"Add ground truth to an existing query\"\"\"\n",
        "\n",
        "    if query_id < 0 or query_id >= len(app_state.chat_history):\n",
        "        return f\"âŒ Invalid query ID. Must be between 0 and {len(app_state.chat_history)-1}\"\n",
        "\n",
        "    query = app_state.chat_history[query_id]\n",
        "    query['ground_truth'] = ground_truth\n",
        "\n",
        "    return f\"\"\"\n",
        "âœ… **Ground Truth Added!**\n",
        "\n",
        "**Query ID:** {query_id}\n",
        "**Question:** {query['question'][:100]}...\n",
        "**Ground Truth:** {ground_truth}\n",
        "\n",
        "Now you can run batch evaluation!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def show_queries_without_ground_truth():\n",
        "    \"\"\"Show all queries that don't have ground truth\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"ğŸ“‹ QUERIES WITHOUT GROUND TRUTH\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    queries_without_gt = []\n",
        "\n",
        "    for i, query in enumerate(app_state.chat_history):\n",
        "        if not query.get('ground_truth') or not str(query.get('ground_truth', '')).strip():\n",
        "            queries_without_gt.append((i, query))\n",
        "\n",
        "    if not queries_without_gt:\n",
        "        print(\"\\nâœ… All queries have ground truth!\")\n",
        "        print(f\"   Total queries: {len(app_state.chat_history)}\")\n",
        "        print(f\"   Ready for batch evaluation!\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFound {len(queries_without_gt)} queries without ground truth:\\n\")\n",
        "\n",
        "    for query_id, query in queries_without_gt:\n",
        "        print(f\"Query ID: {query_id}\")\n",
        "        print(f\"  Question: {query['question'][:80]}...\")\n",
        "        print(f\"  Answer: {query['answer'][:80]}...\")\n",
        "        print(f\"  To add ground truth, use:\")\n",
        "        print(f\"    add_ground_truth_to_query({query_id}, 'your ground truth here')\")\n",
        "        print()\n",
        "\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "def prepare_for_batch_evaluation():\n",
        "    \"\"\"Quick way to prepare queries for batch evaluation\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"ğŸ¯ BATCH EVALUATION PREPARATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    total = len(app_state.chat_history)\n",
        "    with_gt = len([q for q in app_state.chat_history if q.get('ground_truth')])\n",
        "\n",
        "    print(f\"\\nğŸ“Š Current Status:\")\n",
        "    print(f\"   Total queries: {total}\")\n",
        "    print(f\"   With ground truth: {with_gt}\")\n",
        "    print(f\"   Without ground truth: {total - with_gt}\")\n",
        "\n",
        "    if with_gt == 0:\n",
        "        print(f\"\\nâš ï¸ NO GROUND TRUTH DATA\")\n",
        "        print(f\"\\nğŸ’¡ Two options:\")\n",
        "        print(f\"\\n1ï¸âƒ£ Submit new queries with ground truth:\")\n",
        "        print(f\"   â€¢ Go to 'Ask & Evaluate' tab\")\n",
        "        print(f\"   â€¢ Check 'Evaluate with DeepEval'\")\n",
        "        print(f\"   â€¢ Enter ground truth answer\")\n",
        "        print(f\"   â€¢ Submit 3-5 queries\")\n",
        "        print(f\"\\n2ï¸âƒ£ Add ground truth to existing queries:\")\n",
        "        print(f\"   â€¢ Run: show_queries_without_ground_truth()\")\n",
        "        print(f\"   â€¢ For each query, run:\")\n",
        "        print(f\"     add_ground_truth_to_query(query_id, 'correct answer')\")\n",
        "\n",
        "    elif with_gt < 3:\n",
        "        print(f\"\\nâš ï¸ NEED MORE DATA\")\n",
        "        print(f\"   You have {with_gt} queries with ground truth\")\n",
        "        print(f\"   Recommended: At least 3-5 queries\")\n",
        "        print(f\"\\nğŸ’¡ Add {3 - with_gt} more queries with ground truth\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\nâœ… READY FOR BATCH EVALUATION!\")\n",
        "        print(f\"   You have {with_gt} queries with ground truth\")\n",
        "        print(f\"   Go to 'Batch Evaluation' tab and click 'Run Batch Evaluation'\")\n",
        "\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… GROUND TRUTH HELPERS LOADED!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nğŸ“š Available Functions:\")\n",
        "print(\"\\n1ï¸âƒ£ show_queries_without_ground_truth()\")\n",
        "print(\"   Shows all queries that need ground truth\")\n",
        "print(\"\\n2ï¸âƒ£ add_ground_truth_to_query(query_id, 'ground truth text')\")\n",
        "print(\"   Adds ground truth to a specific query\")\n",
        "print(\"\\n3ï¸âƒ£ prepare_for_batch_evaluation()\")\n",
        "print(\"   Shows status and next steps\")\n",
        "print(\"\\nğŸ’¡ Quick Start:\")\n",
        "print(\"   >>> prepare_for_batch_evaluation()\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Auto-check status\n",
        "print(\"\\nğŸ” Auto-checking current status...\")\n",
        "prepare_for_batch_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axlbHuEG_CjI",
        "outputId": "73e89d21-7c3a-484b-c271-5c4f4c27c8c6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "âœ… GROUND TRUTH HELPERS LOADED!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“š Available Functions:\n",
            "\n",
            "1ï¸âƒ£ show_queries_without_ground_truth()\n",
            "   Shows all queries that need ground truth\n",
            "\n",
            "2ï¸âƒ£ add_ground_truth_to_query(query_id, 'ground truth text')\n",
            "   Adds ground truth to a specific query\n",
            "\n",
            "3ï¸âƒ£ prepare_for_batch_evaluation()\n",
            "   Shows status and next steps\n",
            "\n",
            "ğŸ’¡ Quick Start:\n",
            "   >>> prepare_for_batch_evaluation()\n",
            "======================================================================\n",
            "\n",
            "ğŸ” Auto-checking current status...\n",
            "======================================================================\n",
            "ğŸ¯ BATCH EVALUATION PREPARATION\n",
            "======================================================================\n",
            "\n",
            "ğŸ“Š Current Status:\n",
            "   Total queries: 0\n",
            "   With ground truth: 0\n",
            "   Without ground truth: 0\n",
            "\n",
            "âš ï¸ NO GROUND TRUTH DATA\n",
            "\n",
            "ğŸ’¡ Two options:\n",
            "\n",
            "1ï¸âƒ£ Submit new queries with ground truth:\n",
            "   â€¢ Go to 'Ask & Evaluate' tab\n",
            "   â€¢ Check 'Evaluate with DeepEval'\n",
            "   â€¢ Enter ground truth answer\n",
            "   â€¢ Submit 3-5 queries\n",
            "\n",
            "2ï¸âƒ£ Add ground truth to existing queries:\n",
            "   â€¢ Run: show_queries_without_ground_truth()\n",
            "   â€¢ For each query, run:\n",
            "     add_ground_truth_to_query(query_id, 'correct answer')\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =============================================================================\n",
        "# SECTION 10: MAIN EXECUTION\n",
        "# =========================================================="
      ],
      "metadata": {
        "id": "S9GoQjfIPQc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#=========================================================================================================="
      ],
      "metadata": {
        "id": "uTzJ8V83Weia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 1: HELPER FUNCTIONS FOR GRADIO INTERFACE\n"
      ],
      "metadata": {
        "id": "ZmtlpImxyA9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 10 - CELL 1: FIXED HELPER FUNCTIONS FOR GRADIO INTERFACE\n",
        "# Replace your entire helper functions cell with this\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def list_files(folder_path):\n",
        "    \"\"\"List all supported files in a folder\"\"\"\n",
        "\n",
        "    if not os.path.exists(folder_path):\n",
        "        return f\"âŒ Folder not found: {folder_path}\", None\n",
        "\n",
        "    supported_formats = ['.pdf', '.docx', '.txt', '.md']\n",
        "    files_found = []\n",
        "\n",
        "    try:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                if any(file.endswith(ext) for ext in supported_formats):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    file_size = os.path.getsize(file_path) / 1024  # KB\n",
        "\n",
        "                    files_found.append({\n",
        "                        'File Name': file,\n",
        "                        'Type': os.path.splitext(file)[1],\n",
        "                        'Size (KB)': f\"{file_size:.2f}\",\n",
        "                        'Path': file_path\n",
        "                    })\n",
        "\n",
        "        if not files_found:\n",
        "            return f\"ğŸ“‚ No supported files found in: {folder_path}\", None\n",
        "\n",
        "        df = pd.DataFrame(files_found)\n",
        "        message = f\"âœ… Found {len(files_found)} files in: {folder_path}\"\n",
        "\n",
        "        # Store file paths in app_state\n",
        "        app_state.file_paths = [item['Path'] for item in files_found]\n",
        "\n",
        "        return message, df\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error: {str(e)}\", None\n",
        "\n",
        "\n",
        "def upload_and_list(uploaded_files, target_folder):\n",
        "    \"\"\"Handle file uploads and save to target folder\"\"\"\n",
        "\n",
        "    if uploaded_files is None or len(uploaded_files) == 0:\n",
        "        return \"âŒ No files uploaded\", None\n",
        "\n",
        "    os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "    saved_files = []\n",
        "\n",
        "    try:\n",
        "        for file in uploaded_files:\n",
        "            # Copy file to target folder\n",
        "            dest_path = os.path.join(target_folder, os.path.basename(file))\n",
        "            shutil.copy2(file, dest_path)\n",
        "\n",
        "            file_size = os.path.getsize(dest_path) / 1024  # KB\n",
        "\n",
        "            saved_files.append({\n",
        "                'File Name': os.path.basename(file),\n",
        "                'Type': os.path.splitext(file)[1],\n",
        "                'Size (KB)': f\"{file_size:.2f}\",\n",
        "                'Path': dest_path\n",
        "            })\n",
        "\n",
        "        # Store paths\n",
        "        app_state.file_paths = [item['Path'] for item in saved_files]\n",
        "\n",
        "        df = pd.DataFrame(saved_files)\n",
        "        message = f\"âœ… Uploaded {len(saved_files)} files to: {target_folder}\"\n",
        "\n",
        "        return message, df\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Upload error: {str(e)}\", None\n",
        "\n",
        "\n",
        "def initialize_from_files():\n",
        "    \"\"\"Initialize RAG system from stored file paths\"\"\"\n",
        "\n",
        "    if not app_state.file_paths:\n",
        "        return \"âŒ No files selected. Please browse or upload files first.\", \"ğŸ”´ **Offline**\", None\n",
        "\n",
        "    try:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"ğŸš€ INITIALIZING RAG SYSTEM\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Initialize pipeline\n",
        "        app_state.rag_pipeline = RAGPipeline()\n",
        "\n",
        "        # Ingest documents\n",
        "        app_state.rag_pipeline.ingest_documents(app_state.file_paths)\n",
        "\n",
        "        # Update state\n",
        "        app_state.is_initialized = True\n",
        "        app_state.documents_loaded = len(app_state.file_paths)\n",
        "\n",
        "        # Get statistics\n",
        "        stats = get_system_statistics()\n",
        "\n",
        "        message = f\"\"\"\n",
        "## âœ… System Initialized Successfully!\n",
        "\n",
        "**Documents Loaded:** {app_state.documents_loaded}\n",
        "**Status:** ğŸŸ¢ Online and Ready\n",
        "\n",
        "You can now ask questions in the \"Ask & Evaluate\" tab.\n",
        "\"\"\"\n",
        "\n",
        "        return message, \"ğŸŸ¢ **Online**\", stats\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_msg = f\"\"\"\n",
        "## âŒ Initialization Failed\n",
        "\n",
        "**Error:** {str(e)}\n",
        "\n",
        "**Details:**\n",
        "```\n",
        "{traceback.format_exc()}\n",
        "```\n",
        "\"\"\"\n",
        "        return error_msg, \"ğŸ”´ **Offline**\", None\n",
        "\n",
        "\n",
        "def get_system_statistics():\n",
        "    \"\"\"Get system statistics as DataFrame\"\"\"\n",
        "\n",
        "    try:\n",
        "        if not app_state.is_initialized:\n",
        "            return pd.DataFrame({\n",
        "                'Metric': ['System Status'],\n",
        "                'Value': ['Not Initialized']\n",
        "            })\n",
        "\n",
        "        # Safely get total_chunks\n",
        "        total_chunks = getattr(app_state, 'total_chunks', 0)\n",
        "\n",
        "        stats = {\n",
        "            'Metric': [\n",
        "                'Documents Loaded',\n",
        "                'Total Chunks',\n",
        "                'Queries Processed',\n",
        "                'Feedback Received',\n",
        "                'Evaluations Run',\n",
        "                'Ground Truth Pairs'\n",
        "            ],\n",
        "            'Value': [\n",
        "                app_state.documents_loaded,\n",
        "                total_chunks,\n",
        "                len(app_state.chat_history),\n",
        "                len(app_state.feedback_data),\n",
        "                len(app_state.evaluation_results),\n",
        "                len(app_state.ground_truth_qa)\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        return pd.DataFrame(stats)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Statistics error: {e}\")\n",
        "        return pd.DataFrame({\n",
        "            'Metric': ['Error'],\n",
        "            'Value': [str(e)]\n",
        "        })\n",
        "\n",
        "\n",
        "def format_chat_history():\n",
        "    \"\"\"Format chat history for display\"\"\"\n",
        "\n",
        "    try:\n",
        "        if not app_state.chat_history:\n",
        "            return \"ğŸ“ No queries yet. Start asking questions in the 'Ask & Evaluate' tab!\"\n",
        "\n",
        "        history_md = \"# ğŸ“œ Query History\\n\\n\"\n",
        "\n",
        "        for i, entry in enumerate(reversed(app_state.chat_history), 1):\n",
        "            # Safely get values\n",
        "            query_id = entry.get('query_id', 'N/A')\n",
        "            timestamp = entry.get('timestamp', 'N/A')\n",
        "            question = entry.get('question', 'N/A')\n",
        "            answer = entry.get('answer', 'N/A')\n",
        "            total_time = entry.get('total_time', 0)\n",
        "\n",
        "            history_md += f\"\"\"\n",
        "---\n",
        "\n",
        "### Query #{query_id} - {timestamp}\n",
        "\n",
        "**â“ Question:**\n",
        "{question}\n",
        "\n",
        "**ğŸ’¡ Answer:**\n",
        "{answer[:300]}...\n",
        "\n",
        "**â±ï¸ Response Time:** {total_time:.3f}s\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "            # Add evaluation scores if available\n",
        "            if entry.get('evaluation_scores'):\n",
        "                scores = entry['evaluation_scores']\n",
        "                history_md += f\"\"\"\n",
        "**ğŸ“Š Evaluation Scores:**\n",
        "\"\"\"\n",
        "                for metric, score in scores.items():\n",
        "                    history_md += f\"- **{metric.replace('_', ' ').title()}:** {score:.3f}\\n\"\n",
        "                history_md += \"\\n\"\n",
        "\n",
        "        return history_md\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_msg = f\"\"\"\n",
        "# âŒ History Error\n",
        "\n",
        "**Error:** {str(e)}\n",
        "\n",
        "**Traceback:**\n",
        "```\n",
        "{traceback.format_exc()}\n",
        "```\n",
        "\"\"\"\n",
        "        return error_msg\n",
        "\n",
        "\n",
        "def clear_history():\n",
        "    \"\"\"Clear chat history\"\"\"\n",
        "\n",
        "    count = len(app_state.chat_history)\n",
        "    app_state.chat_history = []\n",
        "\n",
        "    return f\"âœ… Cleared {count} queries from history\", \"ğŸ“ No queries yet. Start asking questions!\"\n",
        "\n",
        "\n",
        "def get_system_info():\n",
        "    \"\"\"Get system information\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Safely get total_chunks\n",
        "        total_chunks = getattr(app_state, 'total_chunks', 0)\n",
        "\n",
        "        info = f\"\"\"\n",
        "# â„¹ï¸ System Information\n",
        "\n",
        "## ğŸ“Š Status\n",
        "- **System:** {'ğŸŸ¢ Online' if app_state.is_initialized else 'ğŸ”´ Offline'}\n",
        "- **Documents:** {app_state.documents_loaded}\n",
        "- **Chunks:** {total_chunks}\n",
        "- **Queries:** {len(app_state.chat_history)}\n",
        "- **Feedback:** {len(app_state.feedback_data)}\n",
        "\n",
        "## âš™ï¸ Configuration\n",
        "- **Model:** {config.COHERE_MODEL}\n",
        "- **Embedding Model:** {config.COHERE_EMBEDDING_MODEL}\n",
        "- **Rerank Model:** {config.COHERE_RERANK_MODEL}\n",
        "- **Chunk Size:** {config.CHUNK_SIZE}\n",
        "- **Chunk Overlap:** {config.CHUNK_OVERLAP}\n",
        "- **Top K Retrieval:** {config.TOP_K}\n",
        "- **Top K Rerank:** {config.TOP_K_RERANK}\n",
        "\n",
        "## ğŸ“ˆ Performance\n",
        "\"\"\"\n",
        "\n",
        "        if app_state.chat_history:\n",
        "            avg_time = sum(q.get('total_time', 0) for q in app_state.chat_history) / len(app_state.chat_history)\n",
        "            times = [q.get('total_time', 0) for q in app_state.chat_history]\n",
        "            info += f\"\"\"\n",
        "- **Average Response Time:** {avg_time:.3f}s\n",
        "- **Fastest Query:** {min(times):.3f}s\n",
        "- **Slowest Query:** {max(times):.3f}s\n",
        "\"\"\"\n",
        "        else:\n",
        "            info += \"\\n- No queries processed yet\\n\"\n",
        "\n",
        "        # Feedback summary\n",
        "        if app_state.feedback_data:\n",
        "            summary = app_state.get_feedback_summary()\n",
        "            if isinstance(summary, dict):\n",
        "                info += f\"\"\"\n",
        "## ğŸ’­ Feedback Summary\n",
        "- **Satisfaction Rate:** {summary['satisfaction_rate']:.1f}%\n",
        "- **Average Rating:** {summary['average_rating']:.2f}/5.0\n",
        "- **Thumbs Up:** {summary['thumbs_up']}\n",
        "- **Thumbs Down:** {summary['thumbs_down']}\n",
        "- **Corrections:** {summary['corrections']}\n",
        "\"\"\"\n",
        "\n",
        "        return info\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        return f\"\"\"\n",
        "# âŒ System Info Error\n",
        "\n",
        "**Error:** {str(e)}\n",
        "\n",
        "```\n",
        "{traceback.format_exc()}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"âœ… Fixed helper functions loaded successfully!\")\n",
        "print(\"\\nAvailable functions:\")\n",
        "print(\"  âœ… list_files()\")\n",
        "print(\"  âœ… upload_and_list()\")\n",
        "print(\"  âœ… initialize_from_files()\")\n",
        "print(\"  âœ… get_system_statistics()\")\n",
        "print(\"  âœ… format_chat_history()\")\n",
        "print(\"  âœ… clear_history()\")\n",
        "print(\"  âœ… get_system_info()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuJDR3MQx9UJ",
        "outputId": "63326c0a-1dcc-4fea-b6e7-cc1e435ea3cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fixed helper functions loaded successfully!\n",
            "\n",
            "Available functions:\n",
            "  âœ… list_files()\n",
            "  âœ… upload_and_list()\n",
            "  âœ… initialize_from_files()\n",
            "  âœ… get_system_statistics()\n",
            "  âœ… format_chat_history()\n",
            "  âœ… clear_history()\n",
            "  âœ… get_system_info()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 2: FINAL COMPLETE GRADIO INTERFACE WITH DEEPEVAL\n"
      ],
      "metadata": {
        "id": "xQF0IoZHqlVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --upgrade --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbpFe8TG4uPj",
        "outputId": "f14327bb-c60e-4806-f429-573bad851521"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.0/23.0 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create interface\n",
        "demo = gr.Blocks(title=\"RAG System with DeepEval & Human Feedback\")\n",
        "\n",
        "with demo:\n",
        "\n",
        "    # Header\n",
        "    gr.HTML(\"\"\"\n",
        "    <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                color: white; padding: 2rem; border-radius: 10px;\n",
        "                text-align: center; margin-bottom: 2rem;\n",
        "                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\">\n",
        "        <h1>ğŸš€ Enhanced RAG System</h1>\n",
        "        <p style=\"font-size: 1.2rem;\">With DeepEval Evaluation & Human-in-the-Loop Feedback</p>\n",
        "        <p style=\"font-size: 0.9rem; opacity: 0.9;\">Powered by Cohere â€¢ FAISS â€¢ LangChain â€¢ DeepEval</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Hidden state for current query ID\n",
        "    current_query_id = gr.State(None)\n",
        "    current_eval_scores = gr.State(None)\n",
        "\n",
        "    with gr.Tabs():\n",
        "\n",
        "        # =====================================================================\n",
        "        # TAB 1: DOCUMENT SETUP\n",
        "        # =====================================================================\n",
        "        with gr.Tab(\"ğŸ“‚ Document Setup\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## ğŸ“‚ Load Your Documents\n",
        "\n",
        "            Upload or browse for documents to build your knowledge base.\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=3):\n",
        "                    folder_input = gr.Textbox(\n",
        "                        label=\"ğŸ“‚ Folder Path\",\n",
        "                        value=\"/content/\",\n",
        "                        placeholder=\"/content/your-folder\"\n",
        "                    )\n",
        "                with gr.Column(scale=1):\n",
        "                    browse_btn = gr.Button(\"ğŸ” Browse\", variant=\"secondary\")\n",
        "\n",
        "            browse_output = gr.Markdown()\n",
        "            files_table = gr.Dataframe(label=\"ğŸ“„ Available Files\")\n",
        "\n",
        "            browse_btn.click(\n",
        "                fn=list_files,\n",
        "                inputs=[folder_input],\n",
        "                outputs=[browse_output, files_table]\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "\n",
        "            gr.Markdown(\"### ğŸ“¤ Or Upload Files Directly\")\n",
        "\n",
        "            upload_files = gr.File(\n",
        "                label=\"Select Files to Upload\",\n",
        "                file_count=\"multiple\",\n",
        "                type=\"filepath\"\n",
        "            )\n",
        "            upload_btn = gr.Button(\"ğŸ“¤ Upload Files\", variant=\"secondary\")\n",
        "\n",
        "            upload_btn.click(\n",
        "                fn=upload_and_list,\n",
        "                inputs=[upload_files, folder_input],\n",
        "                outputs=[browse_output, files_table]\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### ğŸš€ Initialize RAG System\")\n",
        "\n",
        "            load_btn = gr.Button(\"ğŸš€ Initialize System\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    load_output = gr.Markdown()\n",
        "                with gr.Column():\n",
        "                    system_status = gr.Markdown(\"ğŸ”´ **Offline**\")\n",
        "\n",
        "            stats_table = gr.Dataframe(label=\"ğŸ“Š System Statistics\")\n",
        "\n",
        "            load_btn.click(\n",
        "                fn=initialize_from_files,\n",
        "                inputs=[],\n",
        "                outputs=[load_output, system_status, stats_table]\n",
        "            )\n",
        "\n",
        "        # =====================================================================\n",
        "        # TAB 2: ASK & EVALUATE WITH DEEPEVAL\n",
        "        # =====================================================================\n",
        "        with gr.Tab(\"ğŸ’¬ Ask & Evaluate\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## ğŸ’¬ Query with DeepEval Evaluation\n",
        "\n",
        "            Ask questions and optionally evaluate responses with DeepEval metrics.\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    question_input = gr.Textbox(\n",
        "                        label=\"â“ Your Question\",\n",
        "                        placeholder=\"Example: MFCC signifie quoi?\",\n",
        "                        lines=3\n",
        "                    )\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    use_reranking = gr.Checkbox(\n",
        "                        label=\"ğŸ¯ Enable Reranking\",\n",
        "                        value=True,\n",
        "                        info=\"Use Cohere reranking\"\n",
        "                    )\n",
        "                    top_k = gr.Slider(\n",
        "                        label=\"ğŸ“Š Top-K Results\",\n",
        "                        minimum=1,\n",
        "                        maximum=10,\n",
        "                        value=5,\n",
        "                        step=1,\n",
        "                        info=\"Number of documents to retrieve\"\n",
        "                    )\n",
        "\n",
        "                    evaluate_checkbox = gr.Checkbox(\n",
        "                        label=\"ğŸ”¬ Evaluate with DeepEval\",\n",
        "                        value=False,\n",
        "                        info=\"Run DeepEval metrics (takes 10-20s)\"\n",
        "                    )\n",
        "\n",
        "            ground_truth_input = gr.Textbox(\n",
        "                label=\"âœ… Ground Truth (Optional - for full evaluation)\",\n",
        "                placeholder=\"Example: MFCC est l'acronyme de Mel Frequency Cepstral Coefficients\",\n",
        "                lines=2,\n",
        "                visible=False,\n",
        "                info=\"Provide expected answer for complete metrics (Contextual Precision & Recall)\"\n",
        "            )\n",
        "\n",
        "            # Show/hide ground truth input\n",
        "            evaluate_checkbox.change(\n",
        "                fn=lambda x: gr.update(visible=x),\n",
        "                inputs=[evaluate_checkbox],\n",
        "                outputs=[ground_truth_input]\n",
        "            )\n",
        "\n",
        "            query_btn = gr.Button(\"ğŸ” Get Answer\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            # Answer display\n",
        "            answer_output = gr.Markdown(label=\"ğŸ’¡ Answer\")\n",
        "\n",
        "            # DeepEval Evaluation Report\n",
        "            with gr.Accordion(\"ğŸ“Š DeepEval Evaluation Report\", open=False):\n",
        "                gr.Markdown(\"\"\"\n",
        "                **DeepEval Metrics Explained:**\n",
        "\n",
        "                | Metric | Description | Ground Truth? |\n",
        "                |--------|-------------|---------------|\n",
        "                | **Faithfulness** | Answer is factually grounded in retrieved context | âŒ No |\n",
        "                | **Answer Relevancy** | Answer directly addresses the question | âŒ No |\n",
        "                | **Contextual Precision** | Retrieved documents are relevant to question | âœ… Yes |\n",
        "                | **Contextual Recall** | All relevant information was retrieved | âœ… Yes |\n",
        "\n",
        "                **Score Interpretation:**\n",
        "                - ğŸ† **0.8-1.0**: Excellent\n",
        "                - âœ… **0.7-0.8**: Good\n",
        "                - âš ï¸ **0.5-0.7**: Fair\n",
        "                - âŒ **Below 0.5**: Needs improvement\n",
        "                \"\"\")\n",
        "                evaluation_report = gr.Markdown(\"Run evaluation to see detailed results...\")\n",
        "\n",
        "            # Sources\n",
        "            with gr.Accordion(\"ğŸ“š Source Documents\", open=True):\n",
        "                sources_output = gr.Dataframe(label=\"Retrieved Sources\")\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "\n",
        "            # FEEDBACK SECTION\n",
        "            gr.Markdown(\"### ğŸ’­ Provide Feedback on This Answer\")\n",
        "            gr.Markdown(\"Help improve the system by providing feedback!\")\n",
        "\n",
        "            with gr.Row():\n",
        "                thumbs_up_btn = gr.Button(\"ğŸ‘ Helpful\", size=\"sm\", variant=\"secondary\")\n",
        "                thumbs_down_btn = gr.Button(\"ğŸ‘ Not Helpful\", size=\"sm\", variant=\"secondary\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    rating_slider = gr.Slider(\n",
        "                        label=\"â­ Rate Answer Quality (1-5 stars)\",\n",
        "                        minimum=1,\n",
        "                        maximum=5,\n",
        "                        value=3,\n",
        "                        step=1\n",
        "                    )\n",
        "                with gr.Column():\n",
        "                    rating_btn = gr.Button(\"ğŸ“ Submit Rating\", variant=\"secondary\")\n",
        "\n",
        "            with gr.Accordion(\"âœï¸ Provide Correction (Builds Ground Truth)\", open=False):\n",
        "                gr.Markdown(\"\"\"\n",
        "                **Important:** When you provide a correction, it's automatically added as ground truth\n",
        "                for batch evaluation!\n",
        "                \"\"\")\n",
        "                correction_input = gr.Textbox(\n",
        "                    label=\"âœ… Correct Answer\",\n",
        "                    placeholder=\"Enter the correct answer here...\",\n",
        "                    lines=4\n",
        "                )\n",
        "                correction_comment = gr.Textbox(\n",
        "                    label=\"ğŸ’¬ Additional Comments (Optional)\",\n",
        "                    placeholder=\"Why was the original answer incorrect?\",\n",
        "                    lines=2\n",
        "                )\n",
        "                correction_btn = gr.Button(\"âœ… Submit Correction\", variant=\"primary\")\n",
        "\n",
        "            feedback_status = gr.Markdown()\n",
        "\n",
        "            # Query execution\n",
        "            query_btn.click(\n",
        "                fn=query_with_evaluation,\n",
        "                inputs=[question_input, use_reranking, top_k, evaluate_checkbox, ground_truth_input],\n",
        "                outputs=[answer_output, sources_output, evaluation_report, current_query_id, current_eval_scores]\n",
        "            )\n",
        "\n",
        "            # Feedback buttons\n",
        "            thumbs_up_btn.click(\n",
        "                fn=lambda qid: record_thumbs_feedback(qid, 'thumbs_up'),\n",
        "                inputs=[current_query_id],\n",
        "                outputs=[feedback_status]\n",
        "            )\n",
        "\n",
        "            thumbs_down_btn.click(\n",
        "                fn=lambda qid: record_thumbs_feedback(qid, 'thumbs_down'),\n",
        "                inputs=[current_query_id],\n",
        "                outputs=[feedback_status]\n",
        "            )\n",
        "\n",
        "            rating_btn.click(\n",
        "                fn=record_rating_feedback,\n",
        "                inputs=[current_query_id, rating_slider],\n",
        "                outputs=[feedback_status]\n",
        "            )\n",
        "\n",
        "            correction_btn.click(\n",
        "                fn=record_correction,\n",
        "                inputs=[current_query_id, correction_input, correction_comment],\n",
        "                outputs=[feedback_status]\n",
        "            )\n",
        "\n",
        "            # Example questions\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### ğŸ’¡ Example Questions\")\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    [\"MFCC signifie quoi?\"],\n",
        "                    [\"Qu'est-ce que le spectrogramme?\"],\n",
        "                    [\"Comment calculer les MFCC?\"],\n",
        "                    [\"Quelle est l'utilitÃ© des MFCC?\"],\n",
        "                    [\"What are the main topics in these documents?\"]\n",
        "                ],\n",
        "                inputs=question_input\n",
        "            )\n",
        "\n",
        "        # =====================================================================\n",
        "        # TAB 3: FEEDBACK DASHBOARD\n",
        "        # =====================================================================\n",
        "        with gr.Tab(\"ğŸ“Š Feedback Dashboard\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## ğŸ“Š Human Feedback Analytics\n",
        "\n",
        "            Monitor user feedback and system performance over time.\n",
        "            \"\"\")\n",
        "\n",
        "            refresh_dashboard_btn = gr.Button(\"ğŸ”„ Refresh Dashboard\", variant=\"primary\")\n",
        "            dashboard_output = gr.Markdown(\"Click 'Refresh Dashboard' to see statistics...\")\n",
        "\n",
        "            refresh_dashboard_btn.click(\n",
        "                fn=get_feedback_dashboard,\n",
        "                inputs=[],\n",
        "                outputs=[dashboard_output]\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "\n",
        "            gr.Markdown(\"### ğŸ’¾ Export Data for Analysis\")\n",
        "            gr.Markdown(\"Download all feedback, evaluation results, and ground truth data.\")\n",
        "\n",
        "            export_btn = gr.Button(\"ğŸ“¥ Export All Data\", variant=\"secondary\", size=\"lg\")\n",
        "            export_status = gr.Markdown()\n",
        "\n",
        "            with gr.Row():\n",
        "                feedback_file = gr.File(label=\"ğŸ“‹ Feedback Data (JSON)\")\n",
        "                eval_file = gr.File(label=\"ğŸ“Š Evaluation Results (JSON)\")\n",
        "                gt_file = gr.File(label=\"âœ… Ground Truth (JSON)\")\n",
        "\n",
        "            export_btn.click(\n",
        "                fn=export_all_data,\n",
        "                inputs=[],\n",
        "                outputs=[export_status, feedback_file, eval_file, gt_file]\n",
        "            )\n",
        "\n",
        "        # =====================================================================\n",
        "        # TAB 4: BATCH EVALUATION WITH DEEPEVAL\n",
        "        # =====================================================================\n",
        "        with gr.Tab(\"ğŸ”¬ Batch Evaluation\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## ğŸ”¬ DeepEval Batch Evaluation\n",
        "\n",
        "            Run comprehensive evaluation on all queries with ground truth.\n",
        "            \"\"\")\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### ğŸ“– How Batch Evaluation Works\n",
        "\n",
        "            **Step 1:** Collect queries with ground truth\n",
        "            - Option A: Use \"Evaluate with DeepEval\" checkbox when asking questions\n",
        "            - Option B: Provide corrections using \"Provide Correction\" button\n",
        "\n",
        "            **Step 2:** Click \"Run Batch Evaluation\" below\n",
        "\n",
        "            **Step 3:** System evaluates all queries and calculates average scores\n",
        "\n",
        "            **Step 4:** Get comprehensive report with recommendations\n",
        "\n",
        "            ---\n",
        "\n",
        "            **Minimum Requirement:** At least 1 query with ground truth\n",
        "            **Recommended:** 3-5 queries for meaningful statistics\n",
        "            \"\"\")\n",
        "\n",
        "            # Status check\n",
        "            with gr.Row():\n",
        "                check_status_btn = gr.Button(\"ğŸ” Check Readiness\", variant=\"secondary\")\n",
        "                batch_status_output = gr.Markdown()\n",
        "\n",
        "            check_status_btn.click(\n",
        "                fn=lambda: prepare_for_batch_evaluation() if 'prepare_for_batch_evaluation' in globals() else \"Run helper functions cell first\",\n",
        "                inputs=[],\n",
        "                outputs=[batch_status_output]\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "\n",
        "            batch_eval_btn = gr.Button(\"ğŸš€ Run Batch Evaluation\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            batch_eval_output = gr.Markdown(\"Click 'Run Batch Evaluation' to start...\")\n",
        "            batch_eval_file = gr.File(label=\"ğŸ“„ Download Evaluation Report\")\n",
        "\n",
        "            batch_eval_btn.click(\n",
        "                fn=run_batch_evaluation,\n",
        "                inputs=[],\n",
        "                outputs=[batch_eval_output, batch_eval_file]\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### ğŸ“Š DeepEval Metrics Reference\n",
        "\n",
        "            | Metric | What It Measures | Good Score | Needs Ground Truth? |\n",
        "            |--------|------------------|------------|---------------------|\n",
        "            | **Faithfulness** | Answer is factually consistent with retrieved context | â‰¥ 0.7 | âŒ No |\n",
        "            | **Answer Relevancy** | Answer directly addresses the user's question | â‰¥ 0.7 | âŒ No |\n",
        "            | **Contextual Precision** | Retrieved documents are relevant and useful | â‰¥ 0.7 | âœ… Yes |\n",
        "            | **Contextual Recall** | All relevant information was successfully retrieved | â‰¥ 0.7 | âœ… Yes |\n",
        "\n",
        "            ### ğŸ¯ Score Interpretation\n",
        "\n",
        "            - **ğŸ† 0.8 - 1.0**: Excellent - System is performing very well\n",
        "            - **âœ… 0.7 - 0.8**: Good - System is performing adequately\n",
        "            - **âš ï¸ 0.5 - 0.7**: Fair - System needs some improvement\n",
        "            - **âŒ Below 0.5**: Poor - Significant improvements needed\n",
        "\n",
        "            ### ğŸ’¡ Tips for Better Evaluations\n",
        "\n",
        "            1. **Provide accurate ground truth** - The quality of evaluation depends on good ground truth\n",
        "            2. **Use diverse questions** - Test different types of queries\n",
        "            3. **Evaluate regularly** - Run batch evaluation after every 10-20 queries\n",
        "            4. **Compare over time** - Track if your system is improving\n",
        "            5. **Focus on weak areas** - If a metric is low, investigate why\n",
        "            \"\"\")\n",
        "\n",
        "        # =====================================================================\n",
        "        # TAB 5: QUERY HISTORY\n",
        "        # =====================================================================\n",
        "        with gr.Tab(\"ğŸ“œ History\"):\n",
        "            gr.Markdown(\"## ğŸ“œ Query History & Management\")\n",
        "\n",
        "            with gr.Row():\n",
        "                refresh_history_btn = gr.Button(\"ğŸ”„ Refresh History\", variant=\"secondary\")\n",
        "                clear_history_btn = gr.Button(\"ğŸ—‘ï¸ Clear History\", variant=\"stop\")\n",
        "\n",
        "            history_display = gr.Markdown(\"No queries yet. Start asking questions in the 'Ask & Evaluate' tab!\")\n",
        "            clear_status = gr.Markdown()\n",
        "\n",
        "            refresh_history_btn.click(\n",
        "                fn=format_chat_history,\n",
        "                inputs=[],\n",
        "                outputs=[history_display]\n",
        "            )\n",
        "\n",
        "            clear_history_btn.click(\n",
        "                fn=clear_history,\n",
        "                inputs=[],\n",
        "                outputs=[clear_status, history_display]\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "\n",
        "            gr.Markdown(\"## â„¹ï¸ System Information\")\n",
        "            info_btn = gr.Button(\"ğŸ“Š Show System Info\", variant=\"secondary\")\n",
        "            info_display = gr.Markdown()\n",
        "\n",
        "            info_btn.click(\n",
        "                fn=get_system_info,\n",
        "                inputs=[],\n",
        "                outputs=[info_display]\n",
        "            )\n",
        "\n",
        "        # =====================================================================\n",
        "        # TAB 6: HELP & DOCUMENTATION\n",
        "        # =====================================================================\n",
        "        with gr.Tab(\"â„¹ï¸ Help\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            # ğŸ“– Complete User Guide\n",
        "\n",
        "            ## ğŸš€ Quick Start (5 Minutes)\n",
        "\n",
        "            ### Step 1: Setup Your Documents (2 min)\n",
        "            1. Go to **\"ğŸ“‚ Document Setup\"** tab\n",
        "            2. Click **\"ğŸ” Browse\"** or use **\"ğŸ“¤ Upload Files\"**\n",
        "            3. Click **\"ğŸš€ Initialize System\"**\n",
        "            4. Wait for \"âœ… System Initialized Successfully!\"\n",
        "\n",
        "            ### Step 2: Ask Your First Question (1 min)\n",
        "            1. Go to **\"ğŸ’¬ Ask & Evaluate\"** tab\n",
        "            2. Type a question (e.g., \"MFCC signifie quoi?\")\n",
        "            3. Click **\"ğŸ” Get Answer\"**\n",
        "            4. Review answer and sources\n",
        "\n",
        "            ### Step 3: Try Evaluation (2 min)\n",
        "            1. Check **\"âœ“ Evaluate with DeepEval\"**\n",
        "            2. Enter ground truth answer\n",
        "            3. Click **\"ğŸ” Get Answer\"**\n",
        "            4. View evaluation scores in the report\n",
        "\n",
        "            ---\n",
        "\n",
        "            ## ğŸ“Š Understanding DeepEval Metrics\n",
        "\n",
        "            ### Faithfulness (Always Available)\n",
        "            **Question:** Is the answer based only on information from the documents?\n",
        "\n",
        "            **What it checks:**\n",
        "            - No hallucinations or made-up facts\n",
        "            - All claims are supported by retrieved context\n",
        "            - Answer doesn't contradict the documents\n",
        "\n",
        "            **Example:**\n",
        "            - âœ… Good: \"MFCC stands for Mel Frequency Cepstral Coefficients\" (if this is in the docs)\n",
        "            - âŒ Bad: \"MFCC was invented in 1980 by John Smith\" (if this isn't in the docs)\n",
        "\n",
        "            ### Answer Relevancy (Always Available)\n",
        "            **Question:** Does the answer actually address what was asked?\n",
        "\n",
        "            **What it checks:**\n",
        "            - Answer is on-topic\n",
        "            - No unnecessary information\n",
        "            - Directly responds to the question\n",
        "\n",
        "            **Example for \"What is MFCC?\":**\n",
        "            - âœ… Good: \"MFCC stands for Mel Frequency Cepstral Coefficients, used in audio processing\"\n",
        "            - âŒ Bad: \"Audio processing has many techniques including FFT, spectrograms, and wavelets...\"\n",
        "\n",
        "            ### Contextual Precision (Needs Ground Truth)\n",
        "            **Question:** Did we retrieve the right documents?\n",
        "\n",
        "            **What it checks:**\n",
        "            - Relevant documents ranked higher\n",
        "            - Irrelevant documents ranked lower\n",
        "            - Quality of retrieval system\n",
        "\n",
        "            **Why it matters:** Good precision = less noise, more signal\n",
        "\n",
        "            ### Contextual Recall (Needs Ground Truth)\n",
        "            **Question:** Did we find all the important information?\n",
        "\n",
        "            **What it checks:**\n",
        "            - All necessary facts were retrieved\n",
        "            - No critical information missing\n",
        "            - Completeness of retrieval\n",
        "\n",
        "            **Why it matters:** Good recall = comprehensive answers\n",
        "\n",
        "            ---\n",
        "\n",
        "            ## ğŸ¯ Best Practices\n",
        "\n",
        "            ### For Single Query Evaluation\n",
        "            1. **Always provide ground truth** when evaluating - you get 4 metrics instead of 2\n",
        "            2. **Be specific** in ground truth - \"MFCC = Mel Frequency Cepstral Coefficients\" is better than \"It's an audio thing\"\n",
        "            3. **Check the report** - read why scores are high or low\n",
        "            4. **Provide feedback** - use thumbs up/down to track satisfaction\n",
        "\n",
        "            ### For Batch Evaluation\n",
        "            1. **Collect 3-5 queries minimum** - more data = better insights\n",
        "            2. **Use diverse questions** - test different aspects of your documents\n",
        "            3. **Provide corrections** - builds your ground truth dataset automatically\n",
        "            4. **Run regularly** - track improvement over time\n",
        "            5. **Compare results** - see if changes improve performance\n",
        "\n",
        "            ### For Ground Truth\n",
        "            **Good Ground Truth:**\n",
        "            - Accurate and complete\n",
        "            - Based on your documents\n",
        "            - Clear and concise\n",
        "            - Consistent in quality\n",
        "\n",
        "            **Bad Ground Truth:**\n",
        "            - Too vague: \"It's a thing\"\n",
        "            - Too detailed: Full paragraph when one sentence would do\n",
        "            - Inconsistent: Sometimes detailed, sometimes brief\n",
        "            - Inaccurate: Doesn't match documents\n",
        "\n",
        "            ---\n",
        "\n",
        "            ## ğŸ”§ Troubleshooting\n",
        "\n",
        "            ### \"System Not Initialized\"\n",
        "            **Problem:** Can't ask questions\n",
        "            **Solution:**\n",
        "            1. Go to \"Document Setup\" tab\n",
        "            2. Upload documents or browse for them\n",
        "            3. Click \"Initialize System\"\n",
        "            4. Wait for success message\n",
        "\n",
        "            ### \"Evaluation Failed\" or No Scores\n",
        "            **Problem:** Evaluation returns no scores\n",
        "            **Solutions:**\n",
        "            1. Check console for detailed errors\n",
        "            2. Verify Cohere API key is valid\n",
        "            3. Check internet connection\n",
        "            4. Wait 5 minutes (rate limits)\n",
        "            5. Try without ground truth first\n",
        "\n",
        "            ### \"No Ground Truth Data Available\"\n",
        "            **Problem:** Can't run batch evaluation\n",
        "            **Solutions:**\n",
        "            1. Submit queries with ground truth (check evaluation box)\n",
        "            2. Or provide corrections to existing queries\n",
        "            3. Need at least 1 query with ground truth\n",
        "            4. Use helper: `prepare_for_batch_evaluation()`\n",
        "\n",
        "            ### Evaluation Takes Too Long\n",
        "            **Normal Timing:**\n",
        "            - Single query without ground truth: 5-10 seconds\n",
        "            - Single query with ground truth: 10-20 seconds\n",
        "            - Batch (3 queries): 30-60 seconds\n",
        "            - Batch (5 queries): 50-100 seconds\n",
        "\n",
        "            **If slower:**\n",
        "            - Check internet connection\n",
        "            - Cohere API might be slow (wait and retry)\n",
        "            - Rate limits (wait 5-10 minutes)\n",
        "\n",
        "            ### Low Evaluation Scores\n",
        "            **If Faithfulness is low:**\n",
        "            - System may be hallucinating\n",
        "            - Check retrieved documents quality\n",
        "            - May need better document chunking\n",
        "\n",
        "            **If Answer Relevancy is low:**\n",
        "            - Answers may be off-topic\n",
        "            - Try rephrasing questions\n",
        "            - Check if documents contain relevant info\n",
        "\n",
        "            **If Contextual Precision is low:**\n",
        "            - Retrieval finding wrong documents\n",
        "            - Try adjusting Top-K parameter\n",
        "            - Enable reranking if disabled\n",
        "\n",
        "            **If Contextual Recall is low:**\n",
        "            - Missing important information\n",
        "            - Increase Top-K to retrieve more\n",
        "            - Check if info exists in documents\n",
        "\n",
        "            ---\n",
        "\n",
        "            ## ğŸ’¾ Data Management\n",
        "\n",
        "            ### Exporting Data\n",
        "            Go to \"Feedback Dashboard\" â†’ Click \"Export All Data\"\n",
        "\n",
        "            **You get 3 files:**\n",
        "            1. **feedback_data.json** - All user feedback (thumbs, ratings, corrections)\n",
        "            2. **evaluation_results.json** - All DeepEval scores and results\n",
        "            3. **ground_truth.json** - All questions with correct answers\n",
        "\n",
        "            **Use cases:**\n",
        "            - Analyze trends in Excel/Python\n",
        "            - Track system improvement\n",
        "            - Build ML datasets\n",
        "            - Generate reports\n",
        "\n",
        "            ### Clearing Data\n",
        "            **History Tab** â†’ \"Clear History\" - Removes all queries and feedback\n",
        "\n",
        "            **Warning:** This cannot be undone! Export data first if needed.\n",
        "\n",
        "            ---\n",
        "\n",
        "            ## ğŸ†˜ Getting Help\n",
        "\n",
        "            ### Check Console Output\n",
        "            Most errors show detailed information in the console. Look for:\n",
        "            - âŒ Error messages\n",
        "            - ğŸ“‹ Tracebacks\n",
        "            - âš ï¸ Warnings\n",
        "\n",
        "            ### Common Error Messages\n",
        "\n",
        "            **\"API Key Error\"**\n",
        "            â†’ Get new key: https://dashboard.cohere.com/api-keys\n",
        "\n",
        "            **\"Rate Limit Exceeded\"**\n",
        "            â†’ Wait 5-10 minutes, check usage limits\n",
        "\n",
        "            **\"Context is empty\"**\n",
        "            â†’ Documents not loaded properly, reinitialize\n",
        "\n",
        "            **\"Model generation failed\"**\n",
        "            â†’ Check Cohere API status, verify internet\n",
        "\n",
        "            ---\n",
        "\n",
        "            ## ğŸ“ Support & Resources\n",
        "\n",
        "            **Created by:** Abdelaziz Ouayazza\n",
        "            - ğŸ“§ Email: ouayazza.abdelaziz@gmail.com\n",
        "            - ğŸ’¼ LinkedIn: [Connect](https://www.linkedin.com/in/abdelaziz-ouayazza-61b19227b/)\n",
        "            - ğŸ’» GitHub: [abdelaziz2003vvb](https://github.com/abdelaziz2003vvb)\n",
        "\n",
        "            **Useful Links:**\n",
        "            - [DeepEval Docs](https://docs.confident-ai.com/)\n",
        "            - [Cohere Documentation](https://docs.cohere.com/)\n",
        "            - [LangChain Guide](https://python.langchain.com/)\n",
        "            - [FAISS Documentation](https://faiss.ai/)\n",
        "\n",
        "            ---\n",
        "\n",
        "            **Version:** 2.0 with DeepEval\n",
        "            **Last Updated:** December 2024\n",
        "            **License:** Educational Use\n",
        "            \"\"\")\n",
        "\n",
        "# =============================================================================\n",
        "# LAUNCH\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸš€ LAUNCHING ENHANCED RAG SYSTEM\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nâœ¨ Features:\")\n",
        "print(\"  âœ… Document upload & processing (PDF, DOCX, TXT, MD)\")\n",
        "print(\"  âœ… FAISS vector search with Cohere reranking\")\n",
        "print(\"  âœ… DeepEval evaluation (4 metrics)\")\n",
        "print(\"  âœ… Human feedback collection (thumbs, ratings, corrections)\")\n",
        "print(\"  âœ… Feedback analytics dashboard\")\n",
        "print(\"  âœ… Batch evaluation with comprehensive reports\")\n",
        "print(\"  âœ… Data export (JSON format)\")\n",
        "print(\"  âœ… Complete documentation & help\")\n",
        "print(\"\\nğŸ“Š DeepEval Metrics:\")\n",
        "print(\"  â€¢ Faithfulness - factual consistency\")\n",
        "print(\"  â€¢ Answer Relevancy - addresses question\")\n",
        "print(\"  â€¢ Contextual Precision - retrieval quality (needs GT)\")\n",
        "print(\"  â€¢ Contextual Recall - retrieval completeness (needs GT)\")\n",
        "print(\"\\nğŸ¯ Quick Start:\")\n",
        "print(\"  1. Document Setup â†’ Upload/Browse â†’ Initialize\")\n",
        "print(\"  2. Ask & Evaluate â†’ Enter question â†’ Get Answer\")\n",
        "print(\"  3. Enable evaluation â†’ Add ground truth â†’ Submit\")\n",
        "print(\"  4. Batch Evaluation â†’ Run evaluation â†’ View report\")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# Launch interface\n",
        "demo.launch(\n",
        "    share=True,\n",
        "    debug=False,\n",
        "    show_error=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2fU7LBN0upNo",
        "outputId": "1d9dd71f-9b52-461a-fb0a-bd390fefa8fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸš€ LAUNCHING ENHANCED RAG SYSTEM\n",
            "======================================================================\n",
            "\n",
            "âœ¨ Features:\n",
            "  âœ… Document upload & processing (PDF, DOCX, TXT, MD)\n",
            "  âœ… FAISS vector search with Cohere reranking\n",
            "  âœ… DeepEval evaluation (4 metrics)\n",
            "  âœ… Human feedback collection (thumbs, ratings, corrections)\n",
            "  âœ… Feedback analytics dashboard\n",
            "  âœ… Batch evaluation with comprehensive reports\n",
            "  âœ… Data export (JSON format)\n",
            "  âœ… Complete documentation & help\n",
            "\n",
            "ğŸ“Š DeepEval Metrics:\n",
            "  â€¢ Faithfulness - factual consistency\n",
            "  â€¢ Answer Relevancy - addresses question\n",
            "  â€¢ Contextual Precision - retrieval quality (needs GT)\n",
            "  â€¢ Contextual Recall - retrieval completeness (needs GT)\n",
            "\n",
            "ğŸ¯ Quick Start:\n",
            "  1. Document Setup â†’ Upload/Browse â†’ Initialize\n",
            "  2. Ask & Evaluate â†’ Enter question â†’ Get Answer\n",
            "  3. Enable evaluation â†’ Add ground truth â†’ Submit\n",
            "  4. Batch Evaluation â†’ Run evaluation â†’ View report\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e2de1796c24c54b1c1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e2de1796c24c54b1c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}